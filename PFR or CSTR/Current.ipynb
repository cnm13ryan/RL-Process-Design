{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulator1 import simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(1,), Discrete(2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = simulator()\n",
    "nb_actions = env.action_space.n\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(1,), 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choice(0) - conversions: [0], reactors: []\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 11:22:58.062011  2464 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0906 11:22:58.101700  2464 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0906 11:22:58.135567  2464 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 610\n",
      "Trainable params: 610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(nb_actions, activation = 'linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0906 11:22:58.301159  2464 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0906 11:22:58.302158  2464 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0906 11:22:58.518352  2464 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "    9/5000: episode: 1, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 0.672, mean reward: 0.075 [0.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.256, 0.672], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18/5000: episode: 2, duration: 1.176s, episode steps: 9, steps per second: 8, episode reward: 0.672, mean reward: 0.075 [0.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.256, 0.672], loss: 0.017605, mean_absolute_error: 0.073634, mean_q: 0.029320\n",
      "   27/5000: episode: 3, duration: 0.037s, episode steps: 9, steps per second: 243, episode reward: 0.672, mean reward: 0.075 [0.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.256, 0.672], loss: 0.008766, mean_absolute_error: 0.058058, mean_q: 0.045074\n",
      "   36/5000: episode: 4, duration: 0.035s, episode steps: 9, steps per second: 258, episode reward: 0.672, mean reward: 0.075 [0.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.256, 0.672], loss: 0.011887, mean_absolute_error: 0.066897, mean_q: 0.047201\n",
      "   45/5000: episode: 5, duration: 0.035s, episode steps: 9, steps per second: 261, episode reward: 0.965, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.658 [0.256, 0.965], loss: 0.007092, mean_absolute_error: 0.060572, mean_q: 0.063611\n",
      "   54/5000: episode: 6, duration: 0.030s, episode steps: 9, steps per second: 297, episode reward: 0.672, mean reward: 0.075 [0.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.256, 0.672], loss: 0.007019, mean_absolute_error: 0.059709, mean_q: 0.061060\n",
      "   63/5000: episode: 7, duration: 0.032s, episode steps: 9, steps per second: 285, episode reward: 0.672, mean reward: 0.075 [0.000, 0.414], mean action: 0.000 [0.000, 0.000], mean observation: 0.626 [0.256, 0.672], loss: 0.004665, mean_absolute_error: 0.042154, mean_q: 0.078057\n",
      "   67/5000: episode: 8, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.002712, mean_absolute_error: 0.040616, mean_q: 0.060987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   71/5000: episode: 9, duration: 0.019s, episode steps: 4, steps per second: 209, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.002811, mean_absolute_error: 0.036221, mean_q: 0.062003\n",
      "   75/5000: episode: 10, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.003224, mean_absolute_error: 0.045386, mean_q: 0.083772\n",
      "   79/5000: episode: 11, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.002047, mean_absolute_error: 0.059382, mean_q: 0.108088\n",
      "   83/5000: episode: 12, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.002097, mean_absolute_error: 0.066867, mean_q: 0.129873\n",
      "   87/5000: episode: 13, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001423, mean_absolute_error: 0.071801, mean_q: 0.160461\n",
      "   91/5000: episode: 14, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001200, mean_absolute_error: 0.079400, mean_q: 0.172918\n",
      "   95/5000: episode: 15, duration: 0.018s, episode steps: 4, steps per second: 224, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001186, mean_absolute_error: 0.093874, mean_q: 0.207222\n",
      "   99/5000: episode: 16, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001133, mean_absolute_error: 0.097595, mean_q: 0.214792\n",
      "  103/5000: episode: 17, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001016, mean_absolute_error: 0.096684, mean_q: 0.224745\n",
      "  107/5000: episode: 18, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000850, mean_absolute_error: 0.108895, mean_q: 0.243507\n",
      "  111/5000: episode: 19, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000862, mean_absolute_error: 0.109005, mean_q: 0.245214\n",
      "  115/5000: episode: 20, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000746, mean_absolute_error: 0.110147, mean_q: 0.259482\n",
      "  119/5000: episode: 21, duration: 0.018s, episode steps: 4, steps per second: 218, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000780, mean_absolute_error: 0.109810, mean_q: 0.260538\n",
      "  123/5000: episode: 22, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000874, mean_absolute_error: 0.114414, mean_q: 0.266580\n",
      "  127/5000: episode: 23, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000874, mean_absolute_error: 0.115169, mean_q: 0.267808\n",
      "  131/5000: episode: 24, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000770, mean_absolute_error: 0.121949, mean_q: 0.288669\n",
      "  135/5000: episode: 25, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000665, mean_absolute_error: 0.125866, mean_q: 0.302501\n",
      "  139/5000: episode: 26, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000734, mean_absolute_error: 0.123617, mean_q: 0.295654\n",
      "  143/5000: episode: 27, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000641, mean_absolute_error: 0.131888, mean_q: 0.319712\n",
      "  147/5000: episode: 28, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000706, mean_absolute_error: 0.132828, mean_q: 0.295769\n",
      "  151/5000: episode: 29, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000837, mean_absolute_error: 0.127372, mean_q: 0.299451\n",
      "  155/5000: episode: 30, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000598, mean_absolute_error: 0.133784, mean_q: 0.325486\n",
      "  159/5000: episode: 31, duration: 0.014s, episode steps: 4, steps per second: 282, episode reward: 1.000, mean reward: 0.250 [0.039, 0.360], mean action: 0.750 [0.000, 1.000], mean observation: 0.735 [0.310, 1.000], loss: 0.000676, mean_absolute_error: 0.133138, mean_q: 0.339845\n",
      "  163/5000: episode: 32, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000890, mean_absolute_error: 0.138719, mean_q: 0.323549\n",
      "  167/5000: episode: 33, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000758, mean_absolute_error: 0.141817, mean_q: 0.324253\n",
      "  171/5000: episode: 34, duration: 0.020s, episode steps: 4, steps per second: 202, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000990, mean_absolute_error: 0.144580, mean_q: 0.332086\n",
      "  175/5000: episode: 35, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000422, mean_absolute_error: 0.154858, mean_q: 0.374457\n",
      "  179/5000: episode: 36, duration: 0.019s, episode steps: 4, steps per second: 209, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000994, mean_absolute_error: 0.155914, mean_q: 0.344337\n",
      "  183/5000: episode: 37, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000676, mean_absolute_error: 0.157537, mean_q: 0.361578\n",
      "  187/5000: episode: 38, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000839, mean_absolute_error: 0.170152, mean_q: 0.392282\n",
      "  192/5000: episode: 39, duration: 0.022s, episode steps: 5, steps per second: 223, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000820, mean_absolute_error: 0.165523, mean_q: 0.357701\n",
      "  196/5000: episode: 40, duration: 0.019s, episode steps: 4, steps per second: 210, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000753, mean_absolute_error: 0.170496, mean_q: 0.375057\n",
      "  200/5000: episode: 41, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000869, mean_absolute_error: 0.182370, mean_q: 0.398378\n",
      "  204/5000: episode: 42, duration: 0.019s, episode steps: 4, steps per second: 208, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000557, mean_absolute_error: 0.182655, mean_q: 0.403639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  209/5000: episode: 43, duration: 0.024s, episode steps: 5, steps per second: 209, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000865, mean_absolute_error: 0.188426, mean_q: 0.415586\n",
      "  213/5000: episode: 44, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000911, mean_absolute_error: 0.173964, mean_q: 0.377055\n",
      "  217/5000: episode: 45, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000765, mean_absolute_error: 0.173139, mean_q: 0.398686\n",
      "  222/5000: episode: 46, duration: 0.019s, episode steps: 5, steps per second: 269, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.001070, mean_absolute_error: 0.178631, mean_q: 0.393503\n",
      "  226/5000: episode: 47, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000598, mean_absolute_error: 0.171840, mean_q: 0.405699\n",
      "  230/5000: episode: 48, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000788, mean_absolute_error: 0.181358, mean_q: 0.406154\n",
      "  234/5000: episode: 49, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001177, mean_absolute_error: 0.175994, mean_q: 0.395763\n",
      "  238/5000: episode: 50, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000447, mean_absolute_error: 0.182369, mean_q: 0.421307\n",
      "  242/5000: episode: 51, duration: 0.018s, episode steps: 4, steps per second: 222, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001147, mean_absolute_error: 0.188312, mean_q: 0.428530\n",
      "  246/5000: episode: 52, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000901, mean_absolute_error: 0.193357, mean_q: 0.442642\n",
      "  250/5000: episode: 53, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000980, mean_absolute_error: 0.187652, mean_q: 0.440295\n",
      "  255/5000: episode: 54, duration: 0.021s, episode steps: 5, steps per second: 243, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.001040, mean_absolute_error: 0.189509, mean_q: 0.430105\n",
      "  259/5000: episode: 55, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001214, mean_absolute_error: 0.197608, mean_q: 0.447329\n",
      "  263/5000: episode: 56, duration: 0.019s, episode steps: 4, steps per second: 216, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000867, mean_absolute_error: 0.195080, mean_q: 0.431229\n",
      "  267/5000: episode: 57, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000531, mean_absolute_error: 0.174914, mean_q: 0.398735\n",
      "  271/5000: episode: 58, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000987, mean_absolute_error: 0.180866, mean_q: 0.400244\n",
      "  275/5000: episode: 59, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001242, mean_absolute_error: 0.189107, mean_q: 0.427075\n",
      "  279/5000: episode: 60, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000852, mean_absolute_error: 0.188881, mean_q: 0.432525\n",
      "  283/5000: episode: 61, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000628, mean_absolute_error: 0.182392, mean_q: 0.436436\n",
      "  287/5000: episode: 62, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000816, mean_absolute_error: 0.186715, mean_q: 0.418274\n",
      "  291/5000: episode: 63, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000636, mean_absolute_error: 0.206790, mean_q: 0.488096\n",
      "  295/5000: episode: 64, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000847, mean_absolute_error: 0.200787, mean_q: 0.465308\n",
      "  299/5000: episode: 65, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000826, mean_absolute_error: 0.201182, mean_q: 0.455799\n",
      "  303/5000: episode: 66, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001487, mean_absolute_error: 0.204179, mean_q: 0.452614\n",
      "  307/5000: episode: 67, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000648, mean_absolute_error: 0.183622, mean_q: 0.424608\n",
      "  311/5000: episode: 68, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001262, mean_absolute_error: 0.213258, mean_q: 0.470482\n",
      "  315/5000: episode: 69, duration: 0.025s, episode steps: 4, steps per second: 162, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001066, mean_absolute_error: 0.203528, mean_q: 0.460263\n",
      "  319/5000: episode: 70, duration: 0.019s, episode steps: 4, steps per second: 211, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001418, mean_absolute_error: 0.215310, mean_q: 0.492964\n",
      "  323/5000: episode: 71, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001226, mean_absolute_error: 0.202264, mean_q: 0.453655\n",
      "  327/5000: episode: 72, duration: 0.020s, episode steps: 4, steps per second: 201, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000638, mean_absolute_error: 0.200124, mean_q: 0.469736\n",
      "  331/5000: episode: 73, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000395, mean_absolute_error: 0.204823, mean_q: 0.471434\n",
      "  335/5000: episode: 74, duration: 0.020s, episode steps: 4, steps per second: 203, episode reward: 1.000, mean reward: 0.250 [0.039, 0.360], mean action: 0.750 [0.000, 1.000], mean observation: 0.735 [0.310, 1.000], loss: 0.000829, mean_absolute_error: 0.203816, mean_q: 0.461459\n",
      "  339/5000: episode: 75, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000081, mean_absolute_error: 0.197935, mean_q: 0.457615\n",
      "  343/5000: episode: 76, duration: 0.018s, episode steps: 4, steps per second: 226, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000808, mean_absolute_error: 0.222078, mean_q: 0.501763\n",
      "  347/5000: episode: 77, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001009, mean_absolute_error: 0.214936, mean_q: 0.469432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  351/5000: episode: 78, duration: 0.019s, episode steps: 4, steps per second: 215, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000957, mean_absolute_error: 0.210604, mean_q: 0.470519\n",
      "  355/5000: episode: 79, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000843, mean_absolute_error: 0.219921, mean_q: 0.482086\n",
      "  359/5000: episode: 80, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000983, mean_absolute_error: 0.199332, mean_q: 0.435594\n",
      "  363/5000: episode: 81, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.001428, mean_absolute_error: 0.208220, mean_q: 0.438014\n",
      "  367/5000: episode: 82, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000500, mean_absolute_error: 0.208115, mean_q: 0.453690\n",
      "  371/5000: episode: 83, duration: 0.014s, episode steps: 4, steps per second: 281, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000131, mean_absolute_error: 0.232082, mean_q: 0.513538\n",
      "  376/5000: episode: 84, duration: 0.018s, episode steps: 5, steps per second: 273, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.001131, mean_absolute_error: 0.212644, mean_q: 0.450552\n",
      "  380/5000: episode: 85, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001256, mean_absolute_error: 0.220458, mean_q: 0.468706\n",
      "  384/5000: episode: 86, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000897, mean_absolute_error: 0.236264, mean_q: 0.492304\n",
      "  388/5000: episode: 87, duration: 0.014s, episode steps: 4, steps per second: 281, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000856, mean_absolute_error: 0.242807, mean_q: 0.513817\n",
      "  392/5000: episode: 88, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000164, mean_absolute_error: 0.226907, mean_q: 0.496811\n",
      "  396/5000: episode: 89, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001210, mean_absolute_error: 0.235901, mean_q: 0.492551\n",
      "  400/5000: episode: 90, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001999, mean_absolute_error: 0.227480, mean_q: 0.474332\n",
      "  404/5000: episode: 91, duration: 0.022s, episode steps: 4, steps per second: 181, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000670, mean_absolute_error: 0.239887, mean_q: 0.510908\n",
      "  408/5000: episode: 92, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000668, mean_absolute_error: 0.246414, mean_q: 0.521512\n",
      "  412/5000: episode: 93, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000741, mean_absolute_error: 0.228136, mean_q: 0.479885\n",
      "  416/5000: episode: 94, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001265, mean_absolute_error: 0.241230, mean_q: 0.500942\n",
      "  420/5000: episode: 95, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000485, mean_absolute_error: 0.251219, mean_q: 0.524560\n",
      "  424/5000: episode: 96, duration: 0.015s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000813, mean_absolute_error: 0.240611, mean_q: 0.503587\n",
      "  428/5000: episode: 97, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000589, mean_absolute_error: 0.236348, mean_q: 0.488974\n",
      "  432/5000: episode: 98, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000728, mean_absolute_error: 0.234103, mean_q: 0.483115\n",
      "  436/5000: episode: 99, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000461, mean_absolute_error: 0.247911, mean_q: 0.513864\n",
      "  440/5000: episode: 100, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000836, mean_absolute_error: 0.216399, mean_q: 0.446603\n",
      "  444/5000: episode: 101, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001150, mean_absolute_error: 0.231726, mean_q: 0.476238\n",
      "  448/5000: episode: 102, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000825, mean_absolute_error: 0.257825, mean_q: 0.539357\n",
      "  452/5000: episode: 103, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000862, mean_absolute_error: 0.256285, mean_q: 0.540369\n",
      "  456/5000: episode: 104, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000512, mean_absolute_error: 0.231547, mean_q: 0.490274\n",
      "  460/5000: episode: 105, duration: 0.021s, episode steps: 4, steps per second: 193, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001404, mean_absolute_error: 0.242075, mean_q: 0.492455\n",
      "  464/5000: episode: 106, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001593, mean_absolute_error: 0.245558, mean_q: 0.508114\n",
      "  468/5000: episode: 107, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000681, mean_absolute_error: 0.267291, mean_q: 0.563272\n",
      "  472/5000: episode: 108, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001614, mean_absolute_error: 0.254849, mean_q: 0.530947\n",
      "  476/5000: episode: 109, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000393, mean_absolute_error: 0.270950, mean_q: 0.562514\n",
      "  480/5000: episode: 110, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000333, mean_absolute_error: 0.255242, mean_q: 0.534967\n",
      "  484/5000: episode: 111, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001345, mean_absolute_error: 0.257324, mean_q: 0.530731\n",
      "  488/5000: episode: 112, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000614, mean_absolute_error: 0.248465, mean_q: 0.514682\n",
      "  492/5000: episode: 113, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000744, mean_absolute_error: 0.253268, mean_q: 0.524558\n",
      "  496/5000: episode: 114, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000432, mean_absolute_error: 0.247408, mean_q: 0.518972\n",
      "  500/5000: episode: 115, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001148, mean_absolute_error: 0.282239, mean_q: 0.585503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  504/5000: episode: 116, duration: 0.019s, episode steps: 4, steps per second: 211, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000773, mean_absolute_error: 0.251260, mean_q: 0.519885\n",
      "  508/5000: episode: 117, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000132, mean_absolute_error: 0.257678, mean_q: 0.548009\n",
      "  512/5000: episode: 118, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000850, mean_absolute_error: 0.239433, mean_q: 0.496657\n",
      "  516/5000: episode: 119, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000381, mean_absolute_error: 0.253206, mean_q: 0.532234\n",
      "  520/5000: episode: 120, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001382, mean_absolute_error: 0.262168, mean_q: 0.539260\n",
      "  524/5000: episode: 121, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001062, mean_absolute_error: 0.261113, mean_q: 0.534516\n",
      "  528/5000: episode: 122, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000129, mean_absolute_error: 0.274478, mean_q: 0.577515\n",
      "  532/5000: episode: 123, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000430, mean_absolute_error: 0.252552, mean_q: 0.532217\n",
      "  536/5000: episode: 124, duration: 0.015s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000460, mean_absolute_error: 0.260429, mean_q: 0.547163\n",
      "  540/5000: episode: 125, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000482, mean_absolute_error: 0.289471, mean_q: 0.602728\n",
      "  544/5000: episode: 126, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000148, mean_absolute_error: 0.256210, mean_q: 0.541227\n",
      "  548/5000: episode: 127, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000739, mean_absolute_error: 0.252925, mean_q: 0.524516\n",
      "  552/5000: episode: 128, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000149, mean_absolute_error: 0.255557, mean_q: 0.535181\n",
      "  556/5000: episode: 129, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000764, mean_absolute_error: 0.245620, mean_q: 0.505882\n",
      "  560/5000: episode: 130, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000421, mean_absolute_error: 0.257544, mean_q: 0.532411\n",
      "  564/5000: episode: 131, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000667, mean_absolute_error: 0.276132, mean_q: 0.568254\n",
      "  568/5000: episode: 132, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000713, mean_absolute_error: 0.252052, mean_q: 0.515205\n",
      "  574/5000: episode: 133, duration: 0.022s, episode steps: 6, steps per second: 277, episode reward: 1.000, mean reward: 0.167 [0.000, 0.291], mean action: 0.500 [0.000, 1.000], mean observation: 0.723 [0.256, 1.000], loss: 0.000502, mean_absolute_error: 0.261104, mean_q: 0.539874\n",
      "  578/5000: episode: 134, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000142, mean_absolute_error: 0.267910, mean_q: 0.552258\n",
      "  583/5000: episode: 135, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000085, mean_absolute_error: 0.252173, mean_q: 0.521031\n",
      "  587/5000: episode: 136, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000448, mean_absolute_error: 0.294689, mean_q: 0.600556\n",
      "  591/5000: episode: 137, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001052, mean_absolute_error: 0.246823, mean_q: 0.498765\n",
      "  595/5000: episode: 138, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000444, mean_absolute_error: 0.256240, mean_q: 0.513691\n",
      "  599/5000: episode: 139, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001108, mean_absolute_error: 0.271452, mean_q: 0.545296\n",
      "  603/5000: episode: 140, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000468, mean_absolute_error: 0.244397, mean_q: 0.495644\n",
      "  608/5000: episode: 141, duration: 0.029s, episode steps: 5, steps per second: 174, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000180, mean_absolute_error: 0.258848, mean_q: 0.531237\n",
      "  612/5000: episode: 142, duration: 0.019s, episode steps: 4, steps per second: 215, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000732, mean_absolute_error: 0.255546, mean_q: 0.517813\n",
      "  616/5000: episode: 143, duration: 0.020s, episode steps: 4, steps per second: 201, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000134, mean_absolute_error: 0.278878, mean_q: 0.569606\n",
      "  620/5000: episode: 144, duration: 0.019s, episode steps: 4, steps per second: 209, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000437, mean_absolute_error: 0.271482, mean_q: 0.548678\n",
      "  624/5000: episode: 145, duration: 0.023s, episode steps: 4, steps per second: 172, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000072, mean_absolute_error: 0.277013, mean_q: 0.570060\n",
      "  633/5000: episode: 146, duration: 0.042s, episode steps: 9, steps per second: 215, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000366, mean_absolute_error: 0.248320, mean_q: 0.504281\n",
      "  638/5000: episode: 147, duration: 0.024s, episode steps: 5, steps per second: 208, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000587, mean_absolute_error: 0.266363, mean_q: 0.537453\n",
      "  642/5000: episode: 148, duration: 0.019s, episode steps: 4, steps per second: 215, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000419, mean_absolute_error: 0.267378, mean_q: 0.538548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  646/5000: episode: 149, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000401, mean_absolute_error: 0.253699, mean_q: 0.510213\n",
      "  650/5000: episode: 150, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000375, mean_absolute_error: 0.281542, mean_q: 0.568744\n",
      "  654/5000: episode: 151, duration: 0.019s, episode steps: 4, steps per second: 216, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001072, mean_absolute_error: 0.275291, mean_q: 0.557918\n",
      "  658/5000: episode: 152, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000815, mean_absolute_error: 0.265820, mean_q: 0.530311\n",
      "  662/5000: episode: 153, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000768, mean_absolute_error: 0.289950, mean_q: 0.586448\n",
      "  667/5000: episode: 154, duration: 0.019s, episode steps: 5, steps per second: 267, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000626, mean_absolute_error: 0.252771, mean_q: 0.515454\n",
      "  671/5000: episode: 155, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000114, mean_absolute_error: 0.259413, mean_q: 0.534474\n",
      "  675/5000: episode: 156, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000670, mean_absolute_error: 0.292053, mean_q: 0.595697\n",
      "  679/5000: episode: 157, duration: 0.014s, episode steps: 4, steps per second: 283, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000643, mean_absolute_error: 0.265332, mean_q: 0.542377\n",
      "  683/5000: episode: 158, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000087, mean_absolute_error: 0.257096, mean_q: 0.528702\n",
      "  687/5000: episode: 159, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000395, mean_absolute_error: 0.272636, mean_q: 0.555433\n",
      "  691/5000: episode: 160, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000405, mean_absolute_error: 0.264045, mean_q: 0.534407\n",
      "  696/5000: episode: 161, duration: 0.019s, episode steps: 5, steps per second: 266, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000085, mean_absolute_error: 0.274557, mean_q: 0.554310\n",
      "  705/5000: episode: 162, duration: 0.038s, episode steps: 9, steps per second: 236, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000215, mean_absolute_error: 0.271648, mean_q: 0.546939\n",
      "  709/5000: episode: 163, duration: 0.020s, episode steps: 4, steps per second: 199, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.278165, mean_q: 0.563300\n",
      "  718/5000: episode: 164, duration: 0.037s, episode steps: 9, steps per second: 240, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000789, mean_absolute_error: 0.275777, mean_q: 0.551833\n",
      "  722/5000: episode: 165, duration: 0.019s, episode steps: 4, steps per second: 207, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000066, mean_absolute_error: 0.285585, mean_q: 0.579560\n",
      "  726/5000: episode: 166, duration: 0.020s, episode steps: 4, steps per second: 201, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000057, mean_absolute_error: 0.268737, mean_q: 0.545917\n",
      "  730/5000: episode: 167, duration: 0.020s, episode steps: 4, steps per second: 204, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000064, mean_absolute_error: 0.276702, mean_q: 0.564431\n",
      "  734/5000: episode: 168, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000374, mean_absolute_error: 0.268349, mean_q: 0.545168\n",
      "  739/5000: episode: 169, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000303, mean_absolute_error: 0.250918, mean_q: 0.508998\n",
      "  743/5000: episode: 170, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.240984, mean_q: 0.492187\n",
      "  748/5000: episode: 171, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000866, mean_absolute_error: 0.255720, mean_q: 0.511461\n",
      "  754/5000: episode: 172, duration: 0.024s, episode steps: 6, steps per second: 252, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.755 [0.256, 1.000], loss: 0.000055, mean_absolute_error: 0.245886, mean_q: 0.498925\n",
      "  758/5000: episode: 173, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001040, mean_absolute_error: 0.240218, mean_q: 0.476734\n",
      "  762/5000: episode: 174, duration: 0.014s, episode steps: 4, steps per second: 285, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.277550, mean_q: 0.562757\n",
      "  766/5000: episode: 175, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000762, mean_absolute_error: 0.269717, mean_q: 0.541824\n",
      "  770/5000: episode: 176, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000070, mean_absolute_error: 0.246691, mean_q: 0.501577\n",
      "  774/5000: episode: 177, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000056, mean_absolute_error: 0.258099, mean_q: 0.526243\n",
      "  778/5000: episode: 178, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000397, mean_absolute_error: 0.274862, mean_q: 0.550325\n",
      "  782/5000: episode: 179, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000369, mean_absolute_error: 0.256298, mean_q: 0.519440\n",
      "  786/5000: episode: 180, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000068, mean_absolute_error: 0.254682, mean_q: 0.515810\n",
      "  790/5000: episode: 181, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000730, mean_absolute_error: 0.281628, mean_q: 0.564807\n",
      "  794/5000: episode: 182, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000729, mean_absolute_error: 0.263380, mean_q: 0.522857\n",
      "  798/5000: episode: 183, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000083, mean_absolute_error: 0.281281, mean_q: 0.570347\n",
      "  802/5000: episode: 184, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.039, 0.360], mean action: 0.750 [0.000, 1.000], mean observation: 0.735 [0.310, 1.000], loss: 0.000040, mean_absolute_error: 0.271628, mean_q: 0.552264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  806/5000: episode: 185, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.240069, mean_q: 0.488464\n",
      "  810/5000: episode: 186, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000048, mean_absolute_error: 0.248206, mean_q: 0.504526\n",
      "  819/5000: episode: 187, duration: 0.035s, episode steps: 9, steps per second: 259, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000508, mean_absolute_error: 0.243385, mean_q: 0.489319\n",
      "  823/5000: episode: 188, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000368, mean_absolute_error: 0.258839, mean_q: 0.521709\n",
      "  827/5000: episode: 189, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001048, mean_absolute_error: 0.252937, mean_q: 0.506152\n",
      "  831/5000: episode: 190, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000066, mean_absolute_error: 0.253081, mean_q: 0.518141\n",
      "  835/5000: episode: 191, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000039, mean_absolute_error: 0.263055, mean_q: 0.537854\n",
      "  839/5000: episode: 192, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000365, mean_absolute_error: 0.243446, mean_q: 0.498762\n",
      "  844/5000: episode: 193, duration: 0.018s, episode steps: 5, steps per second: 280, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000307, mean_absolute_error: 0.245566, mean_q: 0.504320\n",
      "  848/5000: episode: 194, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000377, mean_absolute_error: 0.279072, mean_q: 0.568401\n",
      "  852/5000: episode: 195, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000400, mean_absolute_error: 0.250779, mean_q: 0.508154\n",
      "  856/5000: episode: 196, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000067, mean_absolute_error: 0.252737, mean_q: 0.512954\n",
      "  860/5000: episode: 197, duration: 0.019s, episode steps: 4, steps per second: 209, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.285679, mean_q: 0.579389\n",
      "  864/5000: episode: 198, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000382, mean_absolute_error: 0.262317, mean_q: 0.527533\n",
      "  869/5000: episode: 199, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000030, mean_absolute_error: 0.264462, mean_q: 0.539025\n",
      "  873/5000: episode: 200, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000369, mean_absolute_error: 0.262555, mean_q: 0.530426\n",
      "  877/5000: episode: 201, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000709, mean_absolute_error: 0.270926, mean_q: 0.544845\n",
      "  881/5000: episode: 202, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000739, mean_absolute_error: 0.229696, mean_q: 0.457776\n",
      "  885/5000: episode: 203, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000103, mean_absolute_error: 0.253489, mean_q: 0.508547\n",
      "  889/5000: episode: 204, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000365, mean_absolute_error: 0.226451, mean_q: 0.459564\n",
      "  893/5000: episode: 205, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000349, mean_absolute_error: 0.257056, mean_q: 0.527537\n",
      "  897/5000: episode: 206, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000987, mean_absolute_error: 0.246656, mean_q: 0.497305\n",
      "  901/5000: episode: 207, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000391, mean_absolute_error: 0.248100, mean_q: 0.505282\n",
      "  905/5000: episode: 208, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000661, mean_absolute_error: 0.287876, mean_q: 0.585894\n",
      "  909/5000: episode: 209, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000097, mean_absolute_error: 0.244390, mean_q: 0.497975\n",
      "  913/5000: episode: 210, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000063, mean_absolute_error: 0.273079, mean_q: 0.557901\n",
      "  917/5000: episode: 211, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000704, mean_absolute_error: 0.259648, mean_q: 0.527694\n",
      "  921/5000: episode: 212, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000395, mean_absolute_error: 0.253583, mean_q: 0.512499\n",
      "  925/5000: episode: 213, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000365, mean_absolute_error: 0.268379, mean_q: 0.540846\n",
      "  929/5000: episode: 214, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000372, mean_absolute_error: 0.271675, mean_q: 0.548894\n",
      "  933/5000: episode: 215, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000415, mean_absolute_error: 0.262671, mean_q: 0.523728\n",
      "  937/5000: episode: 216, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000050, mean_absolute_error: 0.272020, mean_q: 0.547767\n",
      "  941/5000: episode: 217, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000043, mean_absolute_error: 0.269649, mean_q: 0.543554\n",
      "  946/5000: episode: 218, duration: 0.018s, episode steps: 5, steps per second: 275, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000596, mean_absolute_error: 0.273817, mean_q: 0.550571\n",
      "  950/5000: episode: 219, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000136, mean_absolute_error: 0.247879, mean_q: 0.497815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  954/5000: episode: 220, duration: 0.023s, episode steps: 4, steps per second: 173, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000063, mean_absolute_error: 0.257768, mean_q: 0.520231\n",
      "  958/5000: episode: 221, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000764, mean_absolute_error: 0.269859, mean_q: 0.531073\n",
      "  962/5000: episode: 222, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.001043, mean_absolute_error: 0.239330, mean_q: 0.482481\n",
      "  966/5000: episode: 223, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000087, mean_absolute_error: 0.253158, mean_q: 0.516679\n",
      "  970/5000: episode: 224, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000339, mean_absolute_error: 0.262778, mean_q: 0.541498\n",
      "  974/5000: episode: 225, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000152, mean_absolute_error: 0.257532, mean_q: 0.528156\n",
      "  978/5000: episode: 226, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000401, mean_absolute_error: 0.264158, mean_q: 0.540704\n",
      "  982/5000: episode: 227, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000111, mean_absolute_error: 0.245694, mean_q: 0.502016\n",
      "  986/5000: episode: 228, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000042, mean_absolute_error: 0.285386, mean_q: 0.583723\n",
      "  990/5000: episode: 229, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000386, mean_absolute_error: 0.258353, mean_q: 0.522858\n",
      "  997/5000: episode: 230, duration: 0.024s, episode steps: 7, steps per second: 296, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000874, mean_absolute_error: 0.280261, mean_q: 0.556971\n",
      " 1001/5000: episode: 231, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000155, mean_absolute_error: 0.263713, mean_q: 0.526217\n",
      " 1005/5000: episode: 232, duration: 0.025s, episode steps: 4, steps per second: 159, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.001053, mean_absolute_error: 0.290907, mean_q: 0.574003\n",
      " 1009/5000: episode: 233, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.001080, mean_absolute_error: 0.266408, mean_q: 0.531625\n",
      " 1013/5000: episode: 234, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000401, mean_absolute_error: 0.236498, mean_q: 0.482619\n",
      " 1017/5000: episode: 235, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000415, mean_absolute_error: 0.272618, mean_q: 0.555886\n",
      " 1021/5000: episode: 236, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000363, mean_absolute_error: 0.269431, mean_q: 0.554190\n",
      " 1025/5000: episode: 237, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000072, mean_absolute_error: 0.251677, mean_q: 0.520274\n",
      " 1029/5000: episode: 238, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000040, mean_absolute_error: 0.262191, mean_q: 0.542769\n",
      " 1033/5000: episode: 239, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000366, mean_absolute_error: 0.240915, mean_q: 0.492803\n",
      " 1037/5000: episode: 240, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000052, mean_absolute_error: 0.255721, mean_q: 0.523309\n",
      " 1041/5000: episode: 241, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.264267, mean_q: 0.536710\n",
      " 1050/5000: episode: 242, duration: 0.032s, episode steps: 9, steps per second: 284, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000186, mean_absolute_error: 0.258551, mean_q: 0.522680\n",
      " 1059/5000: episode: 243, duration: 0.033s, episode steps: 9, steps per second: 270, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000191, mean_absolute_error: 0.250746, mean_q: 0.506003\n",
      " 1064/5000: episode: 244, duration: 0.018s, episode steps: 5, steps per second: 273, episode reward: 1.000, mean reward: 0.200 [0.000, 0.281], mean action: 0.600 [0.000, 1.000], mean observation: 0.637 [0.256, 1.000], loss: 0.000036, mean_absolute_error: 0.261706, mean_q: 0.526567\n",
      " 1073/5000: episode: 245, duration: 0.029s, episode steps: 9, steps per second: 312, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000344, mean_absolute_error: 0.263963, mean_q: 0.532678\n",
      " 1077/5000: episode: 246, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000381, mean_absolute_error: 0.244803, mean_q: 0.495353\n",
      " 1081/5000: episode: 247, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000041, mean_absolute_error: 0.273807, mean_q: 0.558679\n",
      " 1085/5000: episode: 248, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000403, mean_absolute_error: 0.245811, mean_q: 0.499112\n",
      " 1089/5000: episode: 249, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000415, mean_absolute_error: 0.261490, mean_q: 0.526423\n",
      " 1093/5000: episode: 250, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000720, mean_absolute_error: 0.288918, mean_q: 0.585496\n",
      " 1097/5000: episode: 251, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000118, mean_absolute_error: 0.273428, mean_q: 0.553933\n",
      " 1101/5000: episode: 252, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000412, mean_absolute_error: 0.276697, mean_q: 0.560733\n",
      " 1105/5000: episode: 253, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000036, mean_absolute_error: 0.282123, mean_q: 0.574332\n",
      " 1109/5000: episode: 254, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.265133, mean_q: 0.543848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1114/5000: episode: 255, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: 1.000, mean reward: 0.200 [0.000, 0.281], mean action: 0.600 [0.000, 1.000], mean observation: 0.637 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.252252, mean_q: 0.510286\n",
      " 1123/5000: episode: 256, duration: 0.032s, episode steps: 9, steps per second: 282, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000335, mean_absolute_error: 0.264215, mean_q: 0.531799\n",
      " 1131/5000: episode: 257, duration: 0.026s, episode steps: 8, steps per second: 307, episode reward: 1.000, mean reward: 0.125 [0.000, 0.414], mean action: 0.250 [0.000, 1.000], mean observation: 0.842 [0.256, 1.000], loss: 0.000039, mean_absolute_error: 0.249227, mean_q: 0.505276\n",
      " 1135/5000: episode: 258, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000360, mean_absolute_error: 0.270934, mean_q: 0.547891\n",
      " 1139/5000: episode: 259, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.234864, mean_q: 0.475763\n",
      " 1143/5000: episode: 260, duration: 0.015s, episode steps: 4, steps per second: 276, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.262755, mean_q: 0.533344\n",
      " 1148/5000: episode: 261, duration: 0.018s, episode steps: 5, steps per second: 285, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000045, mean_absolute_error: 0.237625, mean_q: 0.481242\n",
      " 1153/5000: episode: 262, duration: 0.018s, episode steps: 5, steps per second: 281, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000031, mean_absolute_error: 0.256135, mean_q: 0.519864\n",
      " 1162/5000: episode: 263, duration: 0.031s, episode steps: 9, steps per second: 288, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000025, mean_absolute_error: 0.250450, mean_q: 0.507089\n",
      " 1166/5000: episode: 264, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.279801, mean_q: 0.564374\n",
      " 1170/5000: episode: 265, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.282307, mean_q: 0.570129\n",
      " 1179/5000: episode: 266, duration: 0.034s, episode steps: 9, steps per second: 263, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000347, mean_absolute_error: 0.264683, mean_q: 0.530740\n",
      " 1183/5000: episode: 267, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000742, mean_absolute_error: 0.248002, mean_q: 0.494928\n",
      " 1188/5000: episode: 268, duration: 0.017s, episode steps: 5, steps per second: 290, episode reward: 1.000, mean reward: 0.200 [0.000, 0.281], mean action: 0.600 [0.000, 1.000], mean observation: 0.637 [0.256, 1.000], loss: 0.000575, mean_absolute_error: 0.254028, mean_q: 0.511198\n",
      " 1192/5000: episode: 269, duration: 0.014s, episode steps: 4, steps per second: 280, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000044, mean_absolute_error: 0.248938, mean_q: 0.509115\n",
      " 1196/5000: episode: 270, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000062, mean_absolute_error: 0.256680, mean_q: 0.525329\n",
      " 1200/5000: episode: 271, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000381, mean_absolute_error: 0.244276, mean_q: 0.496165\n",
      " 1204/5000: episode: 272, duration: 0.014s, episode steps: 4, steps per second: 280, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.265435, mean_q: 0.542338\n",
      " 1208/5000: episode: 273, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000707, mean_absolute_error: 0.254539, mean_q: 0.514193\n",
      " 1212/5000: episode: 274, duration: 0.014s, episode steps: 4, steps per second: 282, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.258214, mean_q: 0.527635\n",
      " 1217/5000: episode: 275, duration: 0.017s, episode steps: 5, steps per second: 289, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.266526, mean_q: 0.542001\n",
      " 1226/5000: episode: 276, duration: 0.037s, episode steps: 9, steps per second: 245, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000480, mean_absolute_error: 0.244925, mean_q: 0.492909\n",
      " 1230/5000: episode: 277, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000733, mean_absolute_error: 0.223150, mean_q: 0.447403\n",
      " 1234/5000: episode: 278, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000384, mean_absolute_error: 0.236763, mean_q: 0.476583\n",
      " 1238/5000: episode: 279, duration: 0.014s, episode steps: 4, steps per second: 283, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000062, mean_absolute_error: 0.248109, mean_q: 0.506855\n",
      " 1242/5000: episode: 280, duration: 0.015s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000357, mean_absolute_error: 0.231038, mean_q: 0.472571\n",
      " 1246/5000: episode: 281, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000381, mean_absolute_error: 0.276248, mean_q: 0.561389\n",
      " 1250/5000: episode: 282, duration: 0.014s, episode steps: 4, steps per second: 280, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000037, mean_absolute_error: 0.249324, mean_q: 0.514359\n",
      " 1254/5000: episode: 283, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000060, mean_absolute_error: 0.252813, mean_q: 0.515651\n",
      " 1258/5000: episode: 284, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000058, mean_absolute_error: 0.245047, mean_q: 0.502584\n",
      " 1262/5000: episode: 285, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000379, mean_absolute_error: 0.250553, mean_q: 0.509397\n",
      " 1267/5000: episode: 286, duration: 0.018s, episode steps: 5, steps per second: 278, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000031, mean_absolute_error: 0.247691, mean_q: 0.502412\n",
      " 1272/5000: episode: 287, duration: 0.018s, episode steps: 5, steps per second: 286, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000302, mean_absolute_error: 0.253725, mean_q: 0.514765\n",
      " 1276/5000: episode: 288, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.248528, mean_q: 0.504453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1285/5000: episode: 289, duration: 0.034s, episode steps: 9, steps per second: 268, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000026, mean_absolute_error: 0.259920, mean_q: 0.528303\n",
      " 1294/5000: episode: 290, duration: 0.031s, episode steps: 9, steps per second: 288, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000186, mean_absolute_error: 0.243216, mean_q: 0.491274\n",
      " 1301/5000: episode: 291, duration: 0.030s, episode steps: 7, steps per second: 235, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000232, mean_absolute_error: 0.237964, mean_q: 0.479480\n",
      " 1305/5000: episode: 292, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000032, mean_absolute_error: 0.264439, mean_q: 0.536436\n",
      " 1309/5000: episode: 293, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.232229, mean_q: 0.470732\n",
      " 1315/5000: episode: 294, duration: 0.021s, episode steps: 6, steps per second: 289, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000255, mean_absolute_error: 0.255226, mean_q: 0.515986\n",
      " 1320/5000: episode: 295, duration: 0.017s, episode steps: 5, steps per second: 289, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000309, mean_absolute_error: 0.240564, mean_q: 0.488550\n",
      " 1324/5000: episode: 296, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.267800, mean_q: 0.543537\n",
      " 1328/5000: episode: 297, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000717, mean_absolute_error: 0.232968, mean_q: 0.468271\n",
      " 1333/5000: episode: 298, duration: 0.017s, episode steps: 5, steps per second: 291, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000308, mean_absolute_error: 0.230066, mean_q: 0.466718\n",
      " 1337/5000: episode: 299, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000722, mean_absolute_error: 0.256928, mean_q: 0.514464\n",
      " 1341/5000: episode: 300, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000070, mean_absolute_error: 0.262113, mean_q: 0.528705\n",
      " 1345/5000: episode: 301, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000401, mean_absolute_error: 0.252440, mean_q: 0.505578\n",
      " 1349/5000: episode: 302, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000412, mean_absolute_error: 0.241145, mean_q: 0.484194\n",
      " 1353/5000: episode: 303, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000401, mean_absolute_error: 0.270895, mean_q: 0.540847\n",
      " 1357/5000: episode: 304, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000090, mean_absolute_error: 0.260914, mean_q: 0.523376\n",
      " 1361/5000: episode: 305, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000429, mean_absolute_error: 0.256610, mean_q: 0.509176\n",
      " 1365/5000: episode: 306, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000390, mean_absolute_error: 0.260280, mean_q: 0.525560\n",
      " 1369/5000: episode: 307, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000045, mean_absolute_error: 0.225215, mean_q: 0.460961\n",
      " 1373/5000: episode: 308, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000033, mean_absolute_error: 0.232470, mean_q: 0.474992\n",
      " 1377/5000: episode: 309, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000374, mean_absolute_error: 0.260432, mean_q: 0.526075\n",
      " 1381/5000: episode: 310, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000352, mean_absolute_error: 0.254549, mean_q: 0.517432\n",
      " 1385/5000: episode: 311, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000040, mean_absolute_error: 0.252222, mean_q: 0.512757\n",
      " 1389/5000: episode: 312, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000733, mean_absolute_error: 0.244344, mean_q: 0.490955\n",
      " 1393/5000: episode: 313, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000065, mean_absolute_error: 0.229889, mean_q: 0.466863\n",
      " 1397/5000: episode: 314, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000390, mean_absolute_error: 0.233854, mean_q: 0.469642\n",
      " 1402/5000: episode: 315, duration: 0.019s, episode steps: 5, steps per second: 269, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000859, mean_absolute_error: 0.264957, mean_q: 0.532640\n",
      " 1406/5000: episode: 316, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000091, mean_absolute_error: 0.242288, mean_q: 0.490970\n",
      " 1410/5000: episode: 317, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000070, mean_absolute_error: 0.268625, mean_q: 0.542146\n",
      " 1414/5000: episode: 318, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000051, mean_absolute_error: 0.258049, mean_q: 0.522693\n",
      " 1418/5000: episode: 319, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000033, mean_absolute_error: 0.265305, mean_q: 0.537472\n",
      " 1425/5000: episode: 320, duration: 0.024s, episode steps: 7, steps per second: 288, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000217, mean_absolute_error: 0.273434, mean_q: 0.553402\n",
      " 1429/5000: episode: 321, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000366, mean_absolute_error: 0.245071, mean_q: 0.493606\n",
      " 1433/5000: episode: 322, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.258183, mean_q: 0.522661\n",
      " 1437/5000: episode: 323, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000012, mean_absolute_error: 0.245024, mean_q: 0.496050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1441/5000: episode: 324, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000028, mean_absolute_error: 0.240749, mean_q: 0.489968\n",
      " 1450/5000: episode: 325, duration: 0.034s, episode steps: 9, steps per second: 265, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000183, mean_absolute_error: 0.253319, mean_q: 0.514473\n",
      " 1454/5000: episode: 326, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.227453, mean_q: 0.461445\n",
      " 1458/5000: episode: 327, duration: 0.019s, episode steps: 4, steps per second: 216, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000386, mean_absolute_error: 0.263774, mean_q: 0.530935\n",
      " 1462/5000: episode: 328, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000038, mean_absolute_error: 0.268949, mean_q: 0.545086\n",
      " 1466/5000: episode: 329, duration: 0.014s, episode steps: 4, steps per second: 280, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000043, mean_absolute_error: 0.256698, mean_q: 0.519954\n",
      " 1473/5000: episode: 330, duration: 0.024s, episode steps: 7, steps per second: 297, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.785 [0.256, 1.000], loss: 0.000244, mean_absolute_error: 0.237429, mean_q: 0.479341\n",
      " 1477/5000: episode: 331, duration: 0.014s, episode steps: 4, steps per second: 280, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000029, mean_absolute_error: 0.267165, mean_q: 0.539184\n",
      " 1481/5000: episode: 332, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.272187, mean_q: 0.548275\n",
      " 1490/5000: episode: 333, duration: 0.031s, episode steps: 9, steps per second: 292, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000024, mean_absolute_error: 0.234357, mean_q: 0.474995\n",
      " 1494/5000: episode: 334, duration: 0.021s, episode steps: 4, steps per second: 187, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.234436, mean_q: 0.474676\n",
      " 1500/5000: episode: 335, duration: 0.022s, episode steps: 6, steps per second: 279, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.273903, mean_q: 0.551620\n",
      " 1504/5000: episode: 336, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.224980, mean_q: 0.455950\n",
      " 1508/5000: episode: 337, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000374, mean_absolute_error: 0.260109, mean_q: 0.522870\n",
      " 1512/5000: episode: 338, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000033, mean_absolute_error: 0.223988, mean_q: 0.452691\n",
      " 1516/5000: episode: 339, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.259929, mean_q: 0.525894\n",
      " 1520/5000: episode: 340, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.241709, mean_q: 0.489268\n",
      " 1529/5000: episode: 341, duration: 0.030s, episode steps: 9, steps per second: 300, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000342, mean_absolute_error: 0.252158, mean_q: 0.506238\n",
      " 1533/5000: episode: 342, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000380, mean_absolute_error: 0.258547, mean_q: 0.520234\n",
      " 1537/5000: episode: 343, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000382, mean_absolute_error: 0.240599, mean_q: 0.481493\n",
      " 1541/5000: episode: 344, duration: 0.019s, episode steps: 4, steps per second: 214, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.254764, mean_q: 0.517013\n",
      " 1545/5000: episode: 345, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.239505, mean_q: 0.491349\n",
      " 1549/5000: episode: 346, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000380, mean_absolute_error: 0.270826, mean_q: 0.545839\n",
      " 1553/5000: episode: 347, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000033, mean_absolute_error: 0.235406, mean_q: 0.478962\n",
      " 1557/5000: episode: 348, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.236237, mean_q: 0.480613\n",
      " 1561/5000: episode: 349, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.258749, mean_q: 0.525667\n",
      " 1570/5000: episode: 350, duration: 0.034s, episode steps: 9, steps per second: 265, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000014, mean_absolute_error: 0.261312, mean_q: 0.529834\n",
      " 1579/5000: episode: 351, duration: 0.031s, episode steps: 9, steps per second: 292, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000006, mean_absolute_error: 0.250116, mean_q: 0.507186\n",
      " 1583/5000: episode: 352, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.254005, mean_q: 0.512679\n",
      " 1587/5000: episode: 353, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000379, mean_absolute_error: 0.247426, mean_q: 0.497396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1596/5000: episode: 354, duration: 0.030s, episode steps: 9, steps per second: 305, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000180, mean_absolute_error: 0.239495, mean_q: 0.482540\n",
      " 1601/5000: episode: 355, duration: 0.021s, episode steps: 5, steps per second: 239, episode reward: 1.000, mean reward: 0.200 [0.000, 0.281], mean action: 0.600 [0.000, 1.000], mean observation: 0.637 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.232330, mean_q: 0.473139\n",
      " 1605/5000: episode: 356, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000798, mean_absolute_error: 0.245550, mean_q: 0.487394\n",
      " 1609/5000: episode: 357, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000084, mean_absolute_error: 0.280993, mean_q: 0.567806\n",
      " 1613/5000: episode: 358, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000057, mean_absolute_error: 0.261493, mean_q: 0.529982\n",
      " 1617/5000: episode: 359, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000041, mean_absolute_error: 0.273280, mean_q: 0.554464\n",
      " 1621/5000: episode: 360, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.237557, mean_q: 0.482483\n",
      " 1625/5000: episode: 361, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000370, mean_absolute_error: 0.240421, mean_q: 0.484500\n",
      " 1629/5000: episode: 362, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.274987, mean_q: 0.556279\n",
      " 1633/5000: episode: 363, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.256640, mean_q: 0.524129\n",
      " 1637/5000: episode: 364, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000040, mean_absolute_error: 0.246440, mean_q: 0.497812\n",
      " 1642/5000: episode: 365, duration: 0.019s, episode steps: 5, steps per second: 266, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.243668, mean_q: 0.494116\n",
      " 1649/5000: episode: 366, duration: 0.028s, episode steps: 7, steps per second: 254, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000448, mean_absolute_error: 0.251719, mean_q: 0.504454\n",
      " 1653/5000: episode: 367, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000056, mean_absolute_error: 0.267183, mean_q: 0.538444\n",
      " 1657/5000: episode: 368, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000050, mean_absolute_error: 0.254045, mean_q: 0.511198\n",
      " 1661/5000: episode: 369, duration: 0.014s, episode steps: 4, steps per second: 283, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000028, mean_absolute_error: 0.247505, mean_q: 0.499287\n",
      " 1665/5000: episode: 370, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.244573, mean_q: 0.493421\n",
      " 1674/5000: episode: 371, duration: 0.031s, episode steps: 9, steps per second: 293, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000179, mean_absolute_error: 0.250767, mean_q: 0.504939\n",
      " 1678/5000: episode: 372, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.254563, mean_q: 0.515487\n",
      " 1682/5000: episode: 373, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000375, mean_absolute_error: 0.267155, mean_q: 0.537205\n",
      " 1687/5000: episode: 374, duration: 0.018s, episode steps: 5, steps per second: 282, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.261610, mean_q: 0.528166\n",
      " 1692/5000: episode: 375, duration: 0.018s, episode steps: 5, steps per second: 281, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000307, mean_absolute_error: 0.249437, mean_q: 0.502455\n",
      " 1696/5000: episode: 376, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000032, mean_absolute_error: 0.241483, mean_q: 0.490039\n",
      " 1701/5000: episode: 377, duration: 0.018s, episode steps: 5, steps per second: 274, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.250507, mean_q: 0.508003\n",
      " 1705/5000: episode: 378, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.233930, mean_q: 0.476126\n",
      " 1714/5000: episode: 379, duration: 0.032s, episode steps: 9, steps per second: 280, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000016, mean_absolute_error: 0.241235, mean_q: 0.487212\n",
      " 1719/5000: episode: 380, duration: 0.018s, episode steps: 5, steps per second: 275, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000935, mean_absolute_error: 0.235303, mean_q: 0.462910\n",
      " 1724/5000: episode: 381, duration: 0.018s, episode steps: 5, steps per second: 273, episode reward: 1.000, mean reward: 0.200 [0.000, 0.281], mean action: 0.600 [0.000, 1.000], mean observation: 0.637 [0.256, 1.000], loss: 0.000603, mean_absolute_error: 0.249287, mean_q: 0.499679\n",
      " 1728/5000: episode: 382, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000051, mean_absolute_error: 0.235766, mean_q: 0.478499\n",
      " 1732/5000: episode: 383, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000047, mean_absolute_error: 0.240681, mean_q: 0.492725\n",
      " 1736/5000: episode: 384, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.260438, mean_q: 0.532552\n",
      " 1740/5000: episode: 385, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000028, mean_absolute_error: 0.257549, mean_q: 0.521363\n",
      " 1744/5000: episode: 386, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000400, mean_absolute_error: 0.239465, mean_q: 0.482416\n",
      " 1748/5000: episode: 387, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000028, mean_absolute_error: 0.254153, mean_q: 0.514977\n",
      " 1753/5000: episode: 388, duration: 0.018s, episode steps: 5, steps per second: 280, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.247477, mean_q: 0.497981\n",
      " 1757/5000: episode: 389, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000027, mean_absolute_error: 0.260086, mean_q: 0.522377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1761/5000: episode: 390, duration: 0.018s, episode steps: 4, steps per second: 222, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000379, mean_absolute_error: 0.220461, mean_q: 0.442103\n",
      " 1765/5000: episode: 391, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.263692, mean_q: 0.532553\n",
      " 1769/5000: episode: 392, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000722, mean_absolute_error: 0.267298, mean_q: 0.533472\n",
      " 1773/5000: episode: 393, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.256543, mean_q: 0.522537\n",
      " 1777/5000: episode: 394, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.232612, mean_q: 0.476832\n",
      " 1781/5000: episode: 395, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.236002, mean_q: 0.480191\n",
      " 1785/5000: episode: 396, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.249945, mean_q: 0.508055\n",
      " 1794/5000: episode: 397, duration: 0.031s, episode steps: 9, steps per second: 289, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000179, mean_absolute_error: 0.248665, mean_q: 0.500402\n",
      " 1798/5000: episode: 398, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.246812, mean_q: 0.496278\n",
      " 1802/5000: episode: 399, duration: 0.014s, episode steps: 4, steps per second: 287, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000062, mean_absolute_error: 0.246374, mean_q: 0.493322\n",
      " 1806/5000: episode: 400, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000710, mean_absolute_error: 0.280277, mean_q: 0.559206\n",
      " 1810/5000: episode: 401, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000421, mean_absolute_error: 0.256844, mean_q: 0.517604\n",
      " 1814/5000: episode: 402, duration: 0.027s, episode steps: 4, steps per second: 146, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000055, mean_absolute_error: 0.248888, mean_q: 0.508628\n",
      " 1818/5000: episode: 403, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000043, mean_absolute_error: 0.214503, mean_q: 0.440815\n",
      " 1822/5000: episode: 404, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000038, mean_absolute_error: 0.251235, mean_q: 0.514497\n",
      " 1827/5000: episode: 405, duration: 0.018s, episode steps: 5, steps per second: 282, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000339, mean_absolute_error: 0.243089, mean_q: 0.491952\n",
      " 1831/5000: episode: 406, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000045, mean_absolute_error: 0.249671, mean_q: 0.508011\n",
      " 1835/5000: episode: 407, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000031, mean_absolute_error: 0.246372, mean_q: 0.501161\n",
      " 1839/5000: episode: 408, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.230295, mean_q: 0.467689\n",
      " 1845/5000: episode: 409, duration: 0.021s, episode steps: 6, steps per second: 291, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.256182, mean_q: 0.517569\n",
      " 1849/5000: episode: 410, duration: 0.014s, episode steps: 4, steps per second: 280, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.264825, mean_q: 0.532909\n",
      " 1853/5000: episode: 411, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000381, mean_absolute_error: 0.245429, mean_q: 0.488547\n",
      " 1859/5000: episode: 412, duration: 0.020s, episode steps: 6, steps per second: 297, episode reward: 1.000, mean reward: 0.167 [0.002, 0.310], mean action: 0.667 [0.000, 1.000], mean observation: 0.692 [0.310, 1.000], loss: 0.000027, mean_absolute_error: 0.247340, mean_q: 0.498759\n",
      " 1864/5000: episode: 413, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000306, mean_absolute_error: 0.249974, mean_q: 0.504611\n",
      " 1868/5000: episode: 414, duration: 0.019s, episode steps: 4, steps per second: 207, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000036, mean_absolute_error: 0.274263, mean_q: 0.556476\n",
      " 1872/5000: episode: 415, duration: 0.020s, episode steps: 4, steps per second: 205, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000056, mean_absolute_error: 0.244124, mean_q: 0.496496\n",
      " 1876/5000: episode: 416, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000698, mean_absolute_error: 0.230583, mean_q: 0.468441\n",
      " 1880/5000: episode: 417, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.269966, mean_q: 0.550271\n",
      " 1885/5000: episode: 418, duration: 0.018s, episode steps: 5, steps per second: 282, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.238291, mean_q: 0.486512\n",
      " 1889/5000: episode: 419, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.254061, mean_q: 0.518936\n",
      " 1893/5000: episode: 420, duration: 0.015s, episode steps: 4, steps per second: 276, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000699, mean_absolute_error: 0.264884, mean_q: 0.532366\n",
      " 1897/5000: episode: 421, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.256606, mean_q: 0.520606\n",
      " 1901/5000: episode: 422, duration: 0.014s, episode steps: 4, steps per second: 284, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.264305, mean_q: 0.533965\n",
      " 1905/5000: episode: 423, duration: 0.014s, episode steps: 4, steps per second: 284, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000367, mean_absolute_error: 0.276327, mean_q: 0.557894\n",
      " 1909/5000: episode: 424, duration: 0.014s, episode steps: 4, steps per second: 283, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.260597, mean_q: 0.530276\n",
      " 1913/5000: episode: 425, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.246639, mean_q: 0.502021\n",
      " 1919/5000: episode: 426, duration: 0.020s, episode steps: 6, steps per second: 293, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000271, mean_absolute_error: 0.229951, mean_q: 0.463849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1924/5000: episode: 427, duration: 0.019s, episode steps: 5, steps per second: 265, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.260749, mean_q: 0.525519\n",
      " 1928/5000: episode: 428, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000026, mean_absolute_error: 0.252456, mean_q: 0.509393\n",
      " 1932/5000: episode: 429, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.254110, mean_q: 0.516162\n",
      " 1936/5000: episode: 430, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.252339, mean_q: 0.512525\n",
      " 1940/5000: episode: 431, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.039, 0.360], mean action: 0.750 [0.000, 1.000], mean observation: 0.735 [0.310, 1.000], loss: 0.000030, mean_absolute_error: 0.257788, mean_q: 0.520287\n",
      " 1944/5000: episode: 432, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.221008, mean_q: 0.447910\n",
      " 1953/5000: episode: 433, duration: 0.031s, episode steps: 9, steps per second: 294, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000193, mean_absolute_error: 0.255284, mean_q: 0.513201\n",
      " 1957/5000: episode: 434, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.242145, mean_q: 0.488599\n",
      " 1961/5000: episode: 435, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000372, mean_absolute_error: 0.258343, mean_q: 0.516804\n",
      " 1966/5000: episode: 436, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000322, mean_absolute_error: 0.245612, mean_q: 0.495711\n",
      " 1970/5000: episode: 437, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000385, mean_absolute_error: 0.250186, mean_q: 0.504279\n",
      " 1974/5000: episode: 438, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000368, mean_absolute_error: 0.252754, mean_q: 0.512582\n",
      " 1978/5000: episode: 439, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000070, mean_absolute_error: 0.236500, mean_q: 0.478611\n",
      " 1983/5000: episode: 440, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000309, mean_absolute_error: 0.261693, mean_q: 0.525619\n",
      " 1987/5000: episode: 441, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000042, mean_absolute_error: 0.246559, mean_q: 0.498227\n",
      " 1992/5000: episode: 442, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000324, mean_absolute_error: 0.243318, mean_q: 0.485352\n",
      " 1996/5000: episode: 443, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000036, mean_absolute_error: 0.258465, mean_q: 0.520770\n",
      " 2000/5000: episode: 444, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000029, mean_absolute_error: 0.203631, mean_q: 0.412500\n",
      " 2004/5000: episode: 445, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.263890, mean_q: 0.532955\n",
      " 2013/5000: episode: 446, duration: 0.032s, episode steps: 9, steps per second: 279, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000019, mean_absolute_error: 0.260758, mean_q: 0.527219\n",
      " 2022/5000: episode: 447, duration: 0.032s, episode steps: 9, steps per second: 279, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000184, mean_absolute_error: 0.226465, mean_q: 0.456538\n",
      " 2026/5000: episode: 448, duration: 0.019s, episode steps: 4, steps per second: 216, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.239854, mean_q: 0.485885\n",
      " 2030/5000: episode: 449, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000381, mean_absolute_error: 0.244607, mean_q: 0.491899\n",
      " 2034/5000: episode: 450, duration: 0.019s, episode steps: 4, steps per second: 214, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.274334, mean_q: 0.555040\n",
      " 2038/5000: episode: 451, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000038, mean_absolute_error: 0.224014, mean_q: 0.453346\n",
      " 2042/5000: episode: 452, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000401, mean_absolute_error: 0.242524, mean_q: 0.487650\n",
      " 2046/5000: episode: 453, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.269068, mean_q: 0.541719\n",
      " 2050/5000: episode: 454, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.001089, mean_absolute_error: 0.251727, mean_q: 0.493825\n",
      " 2054/5000: episode: 455, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000070, mean_absolute_error: 0.247759, mean_q: 0.498759\n",
      " 2058/5000: episode: 456, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000823, mean_absolute_error: 0.254799, mean_q: 0.504761\n",
      " 2062/5000: episode: 457, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000120, mean_absolute_error: 0.273784, mean_q: 0.548719\n",
      " 2066/5000: episode: 458, duration: 0.018s, episode steps: 4, steps per second: 218, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000082, mean_absolute_error: 0.251718, mean_q: 0.507778\n",
      " 2070/5000: episode: 459, duration: 0.020s, episode steps: 4, steps per second: 204, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000060, mean_absolute_error: 0.238433, mean_q: 0.482610\n",
      " 2074/5000: episode: 460, duration: 0.019s, episode steps: 4, steps per second: 206, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000392, mean_absolute_error: 0.279779, mean_q: 0.561827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2078/5000: episode: 461, duration: 0.018s, episode steps: 4, steps per second: 217, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000059, mean_absolute_error: 0.251942, mean_q: 0.504107\n",
      " 2082/5000: episode: 462, duration: 0.019s, episode steps: 4, steps per second: 215, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000376, mean_absolute_error: 0.234705, mean_q: 0.473597\n",
      " 2086/5000: episode: 463, duration: 0.019s, episode steps: 4, steps per second: 215, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000431, mean_absolute_error: 0.230258, mean_q: 0.462285\n",
      " 2091/5000: episode: 464, duration: 0.022s, episode steps: 5, steps per second: 230, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000861, mean_absolute_error: 0.269721, mean_q: 0.536106\n",
      " 2095/5000: episode: 465, duration: 0.018s, episode steps: 4, steps per second: 217, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000066, mean_absolute_error: 0.242151, mean_q: 0.498651\n",
      " 2099/5000: episode: 466, duration: 0.018s, episode steps: 4, steps per second: 222, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000385, mean_absolute_error: 0.220258, mean_q: 0.448521\n",
      " 2103/5000: episode: 467, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000104, mean_absolute_error: 0.232860, mean_q: 0.474562\n",
      " 2107/5000: episode: 468, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000070, mean_absolute_error: 0.243303, mean_q: 0.494919\n",
      " 2111/5000: episode: 469, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000422, mean_absolute_error: 0.260892, mean_q: 0.522081\n",
      " 2115/5000: episode: 470, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000058, mean_absolute_error: 0.232803, mean_q: 0.467672\n",
      " 2119/5000: episode: 471, duration: 0.014s, episode steps: 4, steps per second: 278, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000032, mean_absolute_error: 0.254815, mean_q: 0.512059\n",
      " 2123/5000: episode: 472, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000711, mean_absolute_error: 0.249214, mean_q: 0.499525\n",
      " 2127/5000: episode: 473, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000365, mean_absolute_error: 0.277556, mean_q: 0.558279\n",
      " 2131/5000: episode: 474, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000058, mean_absolute_error: 0.250949, mean_q: 0.508948\n",
      " 2135/5000: episode: 475, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.254558, mean_q: 0.518308\n",
      " 2141/5000: episode: 476, duration: 0.025s, episode steps: 6, steps per second: 236, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.258849, mean_q: 0.526735\n",
      " 2145/5000: episode: 477, duration: 0.020s, episode steps: 4, steps per second: 197, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.281657, mean_q: 0.569532\n",
      " 2149/5000: episode: 478, duration: 0.020s, episode steps: 4, steps per second: 203, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000031, mean_absolute_error: 0.269453, mean_q: 0.545005\n",
      " 2153/5000: episode: 479, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000387, mean_absolute_error: 0.252283, mean_q: 0.503594\n",
      " 2157/5000: episode: 480, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000048, mean_absolute_error: 0.251778, mean_q: 0.511097\n",
      " 2161/5000: episode: 481, duration: 0.019s, episode steps: 4, steps per second: 208, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000377, mean_absolute_error: 0.218009, mean_q: 0.441530\n",
      " 2165/5000: episode: 482, duration: 0.020s, episode steps: 4, steps per second: 203, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.237586, mean_q: 0.484427\n",
      " 2169/5000: episode: 483, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000375, mean_absolute_error: 0.239668, mean_q: 0.483871\n",
      " 2173/5000: episode: 484, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000038, mean_absolute_error: 0.229295, mean_q: 0.466571\n",
      " 2177/5000: episode: 485, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.249477, mean_q: 0.506451\n",
      " 2182/5000: episode: 486, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: 1.000, mean reward: 0.200 [0.000, 0.281], mean action: 0.600 [0.000, 1.000], mean observation: 0.637 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.237680, mean_q: 0.480717\n",
      " 2191/5000: episode: 487, duration: 0.032s, episode steps: 9, steps per second: 280, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000031, mean_absolute_error: 0.237505, mean_q: 0.480781\n",
      " 2200/5000: episode: 488, duration: 0.030s, episode steps: 9, steps per second: 296, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000020, mean_absolute_error: 0.259569, mean_q: 0.524479\n",
      " 2209/5000: episode: 489, duration: 0.030s, episode steps: 9, steps per second: 299, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000019, mean_absolute_error: 0.255240, mean_q: 0.516806\n",
      " 2213/5000: episode: 490, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000376, mean_absolute_error: 0.260226, mean_q: 0.526815\n",
      " 2217/5000: episode: 491, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000738, mean_absolute_error: 0.250699, mean_q: 0.501067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2221/5000: episode: 492, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000373, mean_absolute_error: 0.244981, mean_q: 0.496828\n",
      " 2225/5000: episode: 493, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000032, mean_absolute_error: 0.210522, mean_q: 0.431569\n",
      " 2229/5000: episode: 494, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000018, mean_absolute_error: 0.247655, mean_q: 0.506559\n",
      " 2233/5000: episode: 495, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.224877, mean_q: 0.461210\n",
      " 2237/5000: episode: 496, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.255702, mean_q: 0.518530\n",
      " 2243/5000: episode: 497, duration: 0.021s, episode steps: 6, steps per second: 283, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.755 [0.256, 1.000], loss: 0.000264, mean_absolute_error: 0.268415, mean_q: 0.537538\n",
      " 2247/5000: episode: 498, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000026, mean_absolute_error: 0.263358, mean_q: 0.531668\n",
      " 2251/5000: episode: 499, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.275585, mean_q: 0.554922\n",
      " 2255/5000: episode: 500, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000014, mean_absolute_error: 0.242369, mean_q: 0.492296\n",
      " 2259/5000: episode: 501, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.226196, mean_q: 0.456342\n",
      " 2263/5000: episode: 502, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000393, mean_absolute_error: 0.247544, mean_q: 0.492782\n",
      " 2267/5000: episode: 503, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000045, mean_absolute_error: 0.246598, mean_q: 0.496924\n",
      " 2272/5000: episode: 504, duration: 0.020s, episode steps: 5, steps per second: 244, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000032, mean_absolute_error: 0.265696, mean_q: 0.536583\n",
      " 2276/5000: episode: 505, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.244926, mean_q: 0.496544\n",
      " 2280/5000: episode: 506, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.243029, mean_q: 0.493865\n",
      " 2284/5000: episode: 507, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.241525, mean_q: 0.492688\n",
      " 2288/5000: episode: 508, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000716, mean_absolute_error: 0.249305, mean_q: 0.499640\n",
      " 2292/5000: episode: 509, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000036, mean_absolute_error: 0.253528, mean_q: 0.515415\n",
      " 2296/5000: episode: 510, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.273687, mean_q: 0.550381\n",
      " 2300/5000: episode: 511, duration: 0.022s, episode steps: 4, steps per second: 182, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000383, mean_absolute_error: 0.242482, mean_q: 0.490611\n",
      " 2304/5000: episode: 512, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000364, mean_absolute_error: 0.242581, mean_q: 0.488030\n",
      " 2308/5000: episode: 513, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.260385, mean_q: 0.531412\n",
      " 2312/5000: episode: 514, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.249315, mean_q: 0.507578\n",
      " 2316/5000: episode: 515, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000360, mean_absolute_error: 0.261246, mean_q: 0.528966\n",
      " 2320/5000: episode: 516, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000030, mean_absolute_error: 0.259005, mean_q: 0.525042\n",
      " 2324/5000: episode: 517, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.255488, mean_q: 0.517530\n",
      " 2333/5000: episode: 518, duration: 0.033s, episode steps: 9, steps per second: 269, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000023, mean_absolute_error: 0.260887, mean_q: 0.525789\n",
      " 2337/5000: episode: 519, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.245414, mean_q: 0.495693\n",
      " 2342/5000: episode: 520, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.250047, mean_q: 0.506877\n",
      " 2346/5000: episode: 521, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.251860, mean_q: 0.508983\n",
      " 2350/5000: episode: 522, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000372, mean_absolute_error: 0.263638, mean_q: 0.531204\n",
      " 2354/5000: episode: 523, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000383, mean_absolute_error: 0.243132, mean_q: 0.491744\n",
      " 2358/5000: episode: 524, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.247551, mean_q: 0.504788\n",
      " 2363/5000: episode: 525, duration: 0.018s, episode steps: 5, steps per second: 277, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.259445, mean_q: 0.530113\n",
      " 2367/5000: episode: 526, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000385, mean_absolute_error: 0.279791, mean_q: 0.565134\n",
      " 2371/5000: episode: 527, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000365, mean_absolute_error: 0.241327, mean_q: 0.489222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2375/5000: episode: 528, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.264060, mean_q: 0.533720\n",
      " 2379/5000: episode: 529, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000030, mean_absolute_error: 0.242604, mean_q: 0.492626\n",
      " 2383/5000: episode: 530, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000734, mean_absolute_error: 0.257041, mean_q: 0.512137\n",
      " 2387/5000: episode: 531, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000051, mean_absolute_error: 0.263015, mean_q: 0.533892\n",
      " 2391/5000: episode: 532, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000063, mean_absolute_error: 0.260145, mean_q: 0.524682\n",
      " 2395/5000: episode: 533, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.243965, mean_q: 0.495780\n",
      " 2399/5000: episode: 534, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000032, mean_absolute_error: 0.267976, mean_q: 0.540495\n",
      " 2408/5000: episode: 535, duration: 0.032s, episode steps: 9, steps per second: 280, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000027, mean_absolute_error: 0.254959, mean_q: 0.512908\n",
      " 2415/5000: episode: 536, duration: 0.027s, episode steps: 7, steps per second: 257, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.249449, mean_q: 0.501374\n",
      " 2423/5000: episode: 537, duration: 0.029s, episode steps: 8, steps per second: 274, episode reward: 1.000, mean reward: 0.125 [0.000, 0.310], mean action: 0.500 [0.000, 1.000], mean observation: 0.761 [0.310, 1.000], loss: 0.000022, mean_absolute_error: 0.234503, mean_q: 0.473216\n",
      " 2427/5000: episode: 538, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.235006, mean_q: 0.478448\n",
      " 2431/5000: episode: 539, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.236681, mean_q: 0.479545\n",
      " 2440/5000: episode: 540, duration: 0.032s, episode steps: 9, steps per second: 280, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000181, mean_absolute_error: 0.268534, mean_q: 0.540510\n",
      " 2444/5000: episode: 541, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000045, mean_absolute_error: 0.256961, mean_q: 0.514278\n",
      " 2448/5000: episode: 542, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000771, mean_absolute_error: 0.230034, mean_q: 0.456204\n",
      " 2452/5000: episode: 543, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.231368, mean_q: 0.471848\n",
      " 2456/5000: episode: 544, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000030, mean_absolute_error: 0.241019, mean_q: 0.492319\n",
      " 2461/5000: episode: 545, duration: 0.018s, episode steps: 5, steps per second: 276, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000302, mean_absolute_error: 0.255473, mean_q: 0.520076\n",
      " 2466/5000: episode: 546, duration: 0.019s, episode steps: 5, steps per second: 266, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000317, mean_absolute_error: 0.241966, mean_q: 0.488849\n",
      " 2470/5000: episode: 547, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.247093, mean_q: 0.500633\n",
      " 2474/5000: episode: 548, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000024, mean_absolute_error: 0.249715, mean_q: 0.506064\n",
      " 2478/5000: episode: 549, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.238183, mean_q: 0.483503\n",
      " 2482/5000: episode: 550, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.238972, mean_q: 0.484658\n",
      " 2486/5000: episode: 551, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000006, mean_absolute_error: 0.255452, mean_q: 0.518498\n",
      " 2491/5000: episode: 552, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000335, mean_absolute_error: 0.244061, mean_q: 0.487148\n",
      " 2495/5000: episode: 553, duration: 0.015s, episode steps: 4, steps per second: 276, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.254808, mean_q: 0.512574\n",
      " 2499/5000: episode: 554, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.247952, mean_q: 0.498225\n",
      " 2504/5000: episode: 555, duration: 0.018s, episode steps: 5, steps per second: 274, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000335, mean_absolute_error: 0.260130, mean_q: 0.519870\n",
      " 2508/5000: episode: 556, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000775, mean_absolute_error: 0.247075, mean_q: 0.491584\n",
      " 2512/5000: episode: 557, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000765, mean_absolute_error: 0.229827, mean_q: 0.457471\n",
      " 2516/5000: episode: 558, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000390, mean_absolute_error: 0.223083, mean_q: 0.452157\n",
      " 2520/5000: episode: 559, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.228009, mean_q: 0.468042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2524/5000: episode: 560, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.261518, mean_q: 0.533833\n",
      " 2528/5000: episode: 561, duration: 0.018s, episode steps: 4, steps per second: 217, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.236194, mean_q: 0.481984\n",
      " 2532/5000: episode: 562, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000033, mean_absolute_error: 0.275807, mean_q: 0.557119\n",
      " 2541/5000: episode: 563, duration: 0.030s, episode steps: 9, steps per second: 299, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.250158, mean_q: 0.503844\n",
      " 2545/5000: episode: 564, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.243053, mean_q: 0.490386\n",
      " 2550/5000: episode: 565, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000318, mean_absolute_error: 0.230083, mean_q: 0.460182\n",
      " 2554/5000: episode: 566, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000039, mean_absolute_error: 0.231296, mean_q: 0.464089\n",
      " 2558/5000: episode: 567, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000020, mean_absolute_error: 0.253281, mean_q: 0.511108\n",
      " 2562/5000: episode: 568, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.236452, mean_q: 0.479443\n",
      " 2566/5000: episode: 569, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000387, mean_absolute_error: 0.229360, mean_q: 0.461689\n",
      " 2570/5000: episode: 570, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000029, mean_absolute_error: 0.258758, mean_q: 0.518616\n",
      " 2576/5000: episode: 571, duration: 0.026s, episode steps: 6, steps per second: 232, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.755 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.237395, mean_q: 0.478001\n",
      " 2580/5000: episode: 572, duration: 0.018s, episode steps: 4, steps per second: 222, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.258162, mean_q: 0.519968\n",
      " 2584/5000: episode: 573, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000031, mean_absolute_error: 0.235089, mean_q: 0.473333\n",
      " 2588/5000: episode: 574, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000732, mean_absolute_error: 0.262473, mean_q: 0.521361\n",
      " 2592/5000: episode: 575, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.241098, mean_q: 0.490505\n",
      " 2596/5000: episode: 576, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.239674, mean_q: 0.485466\n",
      " 2600/5000: episode: 577, duration: 0.015s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000391, mean_absolute_error: 0.243274, mean_q: 0.491018\n",
      " 2604/5000: episode: 578, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000041, mean_absolute_error: 0.244139, mean_q: 0.491089\n",
      " 2609/5000: episode: 579, duration: 0.018s, episode steps: 5, steps per second: 281, episode reward: 1.000, mean reward: 0.200 [0.000, 0.281], mean action: 0.600 [0.000, 1.000], mean observation: 0.637 [0.256, 1.000], loss: 0.000048, mean_absolute_error: 0.235369, mean_q: 0.474611\n",
      " 2613/5000: episode: 580, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.244363, mean_q: 0.497927\n",
      " 2617/5000: episode: 581, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.252126, mean_q: 0.512289\n",
      " 2621/5000: episode: 582, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.234485, mean_q: 0.474346\n",
      " 2625/5000: episode: 583, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000045, mean_absolute_error: 0.235475, mean_q: 0.474627\n",
      " 2629/5000: episode: 584, duration: 0.018s, episode steps: 4, steps per second: 222, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000028, mean_absolute_error: 0.260926, mean_q: 0.528886\n",
      " 2633/5000: episode: 585, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000023, mean_absolute_error: 0.269000, mean_q: 0.542211\n",
      " 2637/5000: episode: 586, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.254548, mean_q: 0.514537\n",
      " 2646/5000: episode: 587, duration: 0.031s, episode steps: 9, steps per second: 294, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.256794, mean_q: 0.518707\n",
      " 2650/5000: episode: 588, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000395, mean_absolute_error: 0.251344, mean_q: 0.503229\n",
      " 2654/5000: episode: 589, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000382, mean_absolute_error: 0.241194, mean_q: 0.481054\n",
      " 2658/5000: episode: 590, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000396, mean_absolute_error: 0.256733, mean_q: 0.517264\n",
      " 2662/5000: episode: 591, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000045, mean_absolute_error: 0.262881, mean_q: 0.530255\n",
      " 2667/5000: episode: 592, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.265028, mean_q: 0.538660\n",
      " 2671/5000: episode: 593, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000379, mean_absolute_error: 0.271477, mean_q: 0.547417\n",
      " 2675/5000: episode: 594, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.263316, mean_q: 0.531403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2679/5000: episode: 595, duration: 0.019s, episode steps: 4, steps per second: 211, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.246113, mean_q: 0.495952\n",
      " 2683/5000: episode: 596, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000015, mean_absolute_error: 0.270239, mean_q: 0.544229\n",
      " 2692/5000: episode: 597, duration: 0.033s, episode steps: 9, steps per second: 269, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000014, mean_absolute_error: 0.256732, mean_q: 0.517643\n",
      " 2696/5000: episode: 598, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000017, mean_absolute_error: 0.254546, mean_q: 0.509989\n",
      " 2700/5000: episode: 599, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.234360, mean_q: 0.472524\n",
      " 2705/5000: episode: 600, duration: 0.019s, episode steps: 5, steps per second: 264, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000006, mean_absolute_error: 0.275420, mean_q: 0.556067\n",
      " 2709/5000: episode: 601, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000387, mean_absolute_error: 0.241188, mean_q: 0.485157\n",
      " 2713/5000: episode: 602, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.259347, mean_q: 0.522456\n",
      " 2717/5000: episode: 603, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.257771, mean_q: 0.520094\n",
      " 2721/5000: episode: 604, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000391, mean_absolute_error: 0.246618, mean_q: 0.496688\n",
      " 2725/5000: episode: 605, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000025, mean_absolute_error: 0.250270, mean_q: 0.504837\n",
      " 2729/5000: episode: 606, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.245070, mean_q: 0.494487\n",
      " 2734/5000: episode: 607, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.233530, mean_q: 0.471758\n",
      " 2738/5000: episode: 608, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.223568, mean_q: 0.450106\n",
      " 2742/5000: episode: 609, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.235746, mean_q: 0.471663\n",
      " 2747/5000: episode: 610, duration: 0.018s, episode steps: 5, steps per second: 277, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000340, mean_absolute_error: 0.244836, mean_q: 0.489951\n",
      " 2751/5000: episode: 611, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000032, mean_absolute_error: 0.243755, mean_q: 0.490093\n",
      " 2755/5000: episode: 612, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000054, mean_absolute_error: 0.250670, mean_q: 0.503754\n",
      " 2759/5000: episode: 613, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.245109, mean_q: 0.495137\n",
      " 2763/5000: episode: 614, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000369, mean_absolute_error: 0.279918, mean_q: 0.561964\n",
      " 2767/5000: episode: 615, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000386, mean_absolute_error: 0.243571, mean_q: 0.488922\n",
      " 2772/5000: episode: 616, duration: 0.018s, episode steps: 5, steps per second: 282, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000031, mean_absolute_error: 0.261033, mean_q: 0.525235\n",
      " 2776/5000: episode: 617, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000042, mean_absolute_error: 0.246065, mean_q: 0.495438\n",
      " 2780/5000: episode: 618, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000404, mean_absolute_error: 0.249478, mean_q: 0.500565\n",
      " 2784/5000: episode: 619, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000403, mean_absolute_error: 0.242884, mean_q: 0.486346\n",
      " 2788/5000: episode: 620, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000030, mean_absolute_error: 0.260542, mean_q: 0.524496\n",
      " 2792/5000: episode: 621, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.238726, mean_q: 0.483864\n",
      " 2796/5000: episode: 622, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.239197, mean_q: 0.481399\n",
      " 2800/5000: episode: 623, duration: 0.022s, episode steps: 4, steps per second: 184, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.256787, mean_q: 0.518817\n",
      " 2804/5000: episode: 624, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000004, mean_absolute_error: 0.268653, mean_q: 0.541968\n",
      " 2808/5000: episode: 625, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000380, mean_absolute_error: 0.229600, mean_q: 0.461816\n",
      " 2812/5000: episode: 626, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.251081, mean_q: 0.505094\n",
      " 2816/5000: episode: 627, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000731, mean_absolute_error: 0.250882, mean_q: 0.498314\n",
      " 2820/5000: episode: 628, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000063, mean_absolute_error: 0.244456, mean_q: 0.492030\n",
      " 2824/5000: episode: 629, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000129, mean_absolute_error: 0.253054, mean_q: 0.506545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2828/5000: episode: 630, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000052, mean_absolute_error: 0.265301, mean_q: 0.534206\n",
      " 2832/5000: episode: 631, duration: 0.019s, episode steps: 4, steps per second: 208, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000030, mean_absolute_error: 0.255094, mean_q: 0.513098\n",
      " 2836/5000: episode: 632, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000441, mean_absolute_error: 0.231466, mean_q: 0.458542\n",
      " 2840/5000: episode: 633, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000038, mean_absolute_error: 0.261462, mean_q: 0.525572\n",
      " 2844/5000: episode: 634, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000028, mean_absolute_error: 0.226406, mean_q: 0.455304\n",
      " 2848/5000: episode: 635, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.241942, mean_q: 0.486431\n",
      " 2852/5000: episode: 636, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.223359, mean_q: 0.451470\n",
      " 2856/5000: episode: 637, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000386, mean_absolute_error: 0.277609, mean_q: 0.553916\n",
      " 2860/5000: episode: 638, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000375, mean_absolute_error: 0.249220, mean_q: 0.503022\n",
      " 2864/5000: episode: 639, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000045, mean_absolute_error: 0.254711, mean_q: 0.513298\n",
      " 2868/5000: episode: 640, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000391, mean_absolute_error: 0.259129, mean_q: 0.520664\n",
      " 2872/5000: episode: 641, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000049, mean_absolute_error: 0.237745, mean_q: 0.481069\n",
      " 2876/5000: episode: 642, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.238797, mean_q: 0.482874\n",
      " 2880/5000: episode: 643, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.264922, mean_q: 0.538569\n",
      " 2884/5000: episode: 644, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.258832, mean_q: 0.525034\n",
      " 2888/5000: episode: 645, duration: 0.014s, episode steps: 4, steps per second: 280, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000013, mean_absolute_error: 0.247205, mean_q: 0.499782\n",
      " 2892/5000: episode: 646, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000392, mean_absolute_error: 0.231962, mean_q: 0.467539\n",
      " 2896/5000: episode: 647, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000030, mean_absolute_error: 0.256650, mean_q: 0.513692\n",
      " 2900/5000: episode: 648, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.239663, mean_q: 0.484692\n",
      " 2904/5000: episode: 649, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000741, mean_absolute_error: 0.244607, mean_q: 0.487973\n",
      " 2909/5000: episode: 650, duration: 0.019s, episode steps: 5, steps per second: 265, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.238000, mean_q: 0.483360\n",
      " 2913/5000: episode: 651, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000025, mean_absolute_error: 0.240931, mean_q: 0.487827\n",
      " 2917/5000: episode: 652, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.282190, mean_q: 0.569833\n",
      " 2922/5000: episode: 653, duration: 0.021s, episode steps: 5, steps per second: 241, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.227307, mean_q: 0.458004\n",
      " 2927/5000: episode: 654, duration: 0.021s, episode steps: 5, steps per second: 236, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.250072, mean_q: 0.503490\n",
      " 2931/5000: episode: 655, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.245022, mean_q: 0.495292\n",
      " 2935/5000: episode: 656, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000009, mean_absolute_error: 0.267662, mean_q: 0.541749\n",
      " 2939/5000: episode: 657, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000003, mean_absolute_error: 0.263472, mean_q: 0.535220\n",
      " 2943/5000: episode: 658, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.252938, mean_q: 0.511932\n",
      " 2951/5000: episode: 659, duration: 0.027s, episode steps: 8, steps per second: 297, episode reward: 1.000, mean reward: 0.125 [0.000, 0.414], mean action: 0.250 [0.000, 1.000], mean observation: 0.808 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.240584, mean_q: 0.487063\n",
      " 2956/5000: episode: 660, duration: 0.018s, episode steps: 5, steps per second: 277, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.250201, mean_q: 0.504954\n",
      " 2965/5000: episode: 661, duration: 0.032s, episode steps: 9, steps per second: 284, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.231467, mean_q: 0.468734\n",
      " 2969/5000: episode: 662, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000004, mean_absolute_error: 0.247653, mean_q: 0.501631\n",
      " 2973/5000: episode: 663, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.242443, mean_q: 0.490101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2977/5000: episode: 664, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.238085, mean_q: 0.482022\n",
      " 2981/5000: episode: 665, duration: 0.019s, episode steps: 4, steps per second: 215, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.281429, mean_q: 0.568010\n",
      " 2989/5000: episode: 666, duration: 0.028s, episode steps: 8, steps per second: 281, episode reward: 1.000, mean reward: 0.125 [0.000, 0.414], mean action: 0.250 [0.000, 1.000], mean observation: 0.842 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.250333, mean_q: 0.504916\n",
      " 2994/5000: episode: 667, duration: 0.018s, episode steps: 5, steps per second: 274, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.258996, mean_q: 0.523051\n",
      " 2998/5000: episode: 668, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.238535, mean_q: 0.482973\n",
      " 3005/5000: episode: 669, duration: 0.024s, episode steps: 7, steps per second: 290, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000237, mean_absolute_error: 0.251257, mean_q: 0.504841\n",
      " 3009/5000: episode: 670, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000398, mean_absolute_error: 0.251659, mean_q: 0.500719\n",
      " 3015/5000: episode: 671, duration: 0.022s, episode steps: 6, steps per second: 278, episode reward: 1.000, mean reward: 0.167 [0.000, 0.291], mean action: 0.500 [0.000, 1.000], mean observation: 0.723 [0.256, 1.000], loss: 0.000044, mean_absolute_error: 0.240899, mean_q: 0.484047\n",
      " 3019/5000: episode: 672, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000013, mean_absolute_error: 0.252246, mean_q: 0.512239\n",
      " 3023/5000: episode: 673, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000386, mean_absolute_error: 0.246163, mean_q: 0.497343\n",
      " 3027/5000: episode: 674, duration: 0.014s, episode steps: 4, steps per second: 283, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000407, mean_absolute_error: 0.233889, mean_q: 0.470510\n",
      " 3031/5000: episode: 675, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000046, mean_absolute_error: 0.229734, mean_q: 0.462887\n",
      " 3035/5000: episode: 676, duration: 0.020s, episode steps: 4, steps per second: 200, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000396, mean_absolute_error: 0.267345, mean_q: 0.537018\n",
      " 3039/5000: episode: 677, duration: 0.019s, episode steps: 4, steps per second: 216, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000049, mean_absolute_error: 0.246273, mean_q: 0.493802\n",
      " 3043/5000: episode: 678, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000038, mean_absolute_error: 0.233574, mean_q: 0.470493\n",
      " 3047/5000: episode: 679, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.267070, mean_q: 0.540392\n",
      " 3051/5000: episode: 680, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.265672, mean_q: 0.535674\n",
      " 3060/5000: episode: 681, duration: 0.030s, episode steps: 9, steps per second: 305, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000011, mean_absolute_error: 0.257519, mean_q: 0.517501\n",
      " 3069/5000: episode: 682, duration: 0.029s, episode steps: 9, steps per second: 306, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000015, mean_absolute_error: 0.234402, mean_q: 0.471607\n",
      " 3073/5000: episode: 683, duration: 0.014s, episode steps: 4, steps per second: 281, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000395, mean_absolute_error: 0.252340, mean_q: 0.504679\n",
      " 3077/5000: episode: 684, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.215403, mean_q: 0.434794\n",
      " 3081/5000: episode: 685, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000026, mean_absolute_error: 0.254418, mean_q: 0.510948\n",
      " 3085/5000: episode: 686, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.268783, mean_q: 0.543046\n",
      " 3094/5000: episode: 687, duration: 0.030s, episode steps: 9, steps per second: 303, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000355, mean_absolute_error: 0.261148, mean_q: 0.523857\n",
      " 3098/5000: episode: 688, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000057, mean_absolute_error: 0.249055, mean_q: 0.498488\n",
      " 3102/5000: episode: 689, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000034, mean_absolute_error: 0.218591, mean_q: 0.439951\n",
      " 3106/5000: episode: 690, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.260413, mean_q: 0.522589\n",
      " 3110/5000: episode: 691, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000396, mean_absolute_error: 0.236541, mean_q: 0.472163\n",
      " 3114/5000: episode: 692, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.249081, mean_q: 0.499884\n",
      " 3118/5000: episode: 693, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000382, mean_absolute_error: 0.261558, mean_q: 0.521078\n",
      " 3122/5000: episode: 694, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.246115, mean_q: 0.496366\n",
      " 3126/5000: episode: 695, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.224998, mean_q: 0.455364\n",
      " 3130/5000: episode: 696, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.245982, mean_q: 0.497486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3134/5000: episode: 697, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000055, mean_absolute_error: 0.220248, mean_q: 0.440975\n",
      " 3138/5000: episode: 698, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.260847, mean_q: 0.527094\n",
      " 3142/5000: episode: 699, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000026, mean_absolute_error: 0.245807, mean_q: 0.497323\n",
      " 3146/5000: episode: 700, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.262174, mean_q: 0.528696\n",
      " 3150/5000: episode: 701, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.252664, mean_q: 0.510471\n",
      " 3154/5000: episode: 702, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.248189, mean_q: 0.499619\n",
      " 3160/5000: episode: 703, duration: 0.021s, episode steps: 6, steps per second: 290, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.238715, mean_q: 0.478323\n",
      " 3164/5000: episode: 704, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000020, mean_absolute_error: 0.263249, mean_q: 0.530182\n",
      " 3168/5000: episode: 705, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000007, mean_absolute_error: 0.249016, mean_q: 0.501708\n",
      " 3172/5000: episode: 706, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000006, mean_absolute_error: 0.242750, mean_q: 0.491115\n",
      " 3176/5000: episode: 707, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.242136, mean_q: 0.489015\n",
      " 3180/5000: episode: 708, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000002, mean_absolute_error: 0.255076, mean_q: 0.515786\n",
      " 3187/5000: episode: 709, duration: 0.027s, episode steps: 7, steps per second: 260, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.220867, mean_q: 0.445823\n",
      " 3191/5000: episode: 710, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.284494, mean_q: 0.572741\n",
      " 3195/5000: episode: 711, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000396, mean_absolute_error: 0.236649, mean_q: 0.473938\n",
      " 3199/5000: episode: 712, duration: 0.014s, episode steps: 4, steps per second: 289, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.252412, mean_q: 0.507692\n",
      " 3203/5000: episode: 713, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.249187, mean_q: 0.503340\n",
      " 3207/5000: episode: 714, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000392, mean_absolute_error: 0.245498, mean_q: 0.494790\n",
      " 3211/5000: episode: 715, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.247719, mean_q: 0.498926\n",
      " 3215/5000: episode: 716, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.270256, mean_q: 0.543592\n",
      " 3219/5000: episode: 717, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000385, mean_absolute_error: 0.249501, mean_q: 0.500806\n",
      " 3223/5000: episode: 718, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.260040, mean_q: 0.522192\n",
      " 3227/5000: episode: 719, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.221048, mean_q: 0.446028\n",
      " 3231/5000: episode: 720, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.257696, mean_q: 0.519347\n",
      " 3235/5000: episode: 721, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000011, mean_absolute_error: 0.240438, mean_q: 0.486406\n",
      " 3239/5000: episode: 722, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000006, mean_absolute_error: 0.242603, mean_q: 0.490828\n",
      " 3243/5000: episode: 723, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.237334, mean_q: 0.479082\n",
      " 3247/5000: episode: 724, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000395, mean_absolute_error: 0.232867, mean_q: 0.466635\n",
      " 3252/5000: episode: 725, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000614, mean_absolute_error: 0.239009, mean_q: 0.478147\n",
      " 3257/5000: episode: 726, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000075, mean_absolute_error: 0.237130, mean_q: 0.475830\n",
      " 3261/5000: episode: 727, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000478, mean_absolute_error: 0.243934, mean_q: 0.481357\n",
      " 3265/5000: episode: 728, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000106, mean_absolute_error: 0.277530, mean_q: 0.555181\n",
      " 3269/5000: episode: 729, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000388, mean_absolute_error: 0.253164, mean_q: 0.504596\n",
      " 3273/5000: episode: 730, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000056, mean_absolute_error: 0.233776, mean_q: 0.470891\n",
      " 3277/5000: episode: 731, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000031, mean_absolute_error: 0.274731, mean_q: 0.552811\n",
      " 3286/5000: episode: 732, duration: 0.030s, episode steps: 9, steps per second: 300, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000201, mean_absolute_error: 0.228537, mean_q: 0.457372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3290/5000: episode: 733, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000397, mean_absolute_error: 0.267050, mean_q: 0.529988\n",
      " 3294/5000: episode: 734, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000054, mean_absolute_error: 0.239680, mean_q: 0.483247\n",
      " 3298/5000: episode: 735, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000409, mean_absolute_error: 0.257821, mean_q: 0.517905\n",
      " 3302/5000: episode: 736, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000699, mean_absolute_error: 0.240033, mean_q: 0.481974\n",
      " 3306/5000: episode: 737, duration: 0.019s, episode steps: 4, steps per second: 214, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000035, mean_absolute_error: 0.262468, mean_q: 0.533054\n",
      " 3310/5000: episode: 738, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000030, mean_absolute_error: 0.280854, mean_q: 0.569197\n",
      " 3314/5000: episode: 739, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.244047, mean_q: 0.497202\n",
      " 3318/5000: episode: 740, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.241697, mean_q: 0.491662\n",
      " 3322/5000: episode: 741, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000030, mean_absolute_error: 0.232535, mean_q: 0.470160\n",
      " 3328/5000: episode: 742, duration: 0.022s, episode steps: 6, steps per second: 275, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000266, mean_absolute_error: 0.249800, mean_q: 0.500044\n",
      " 3332/5000: episode: 743, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000022, mean_absolute_error: 0.274288, mean_q: 0.552209\n",
      " 3336/5000: episode: 744, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000032, mean_absolute_error: 0.252997, mean_q: 0.511043\n",
      " 3340/5000: episode: 745, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000392, mean_absolute_error: 0.251827, mean_q: 0.504901\n",
      " 3344/5000: episode: 746, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000030, mean_absolute_error: 0.247624, mean_q: 0.497918\n",
      " 3352/5000: episode: 747, duration: 0.029s, episode steps: 8, steps per second: 280, episode reward: 1.000, mean reward: 0.125 [0.000, 0.414], mean action: 0.250 [0.000, 1.000], mean observation: 0.808 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.250015, mean_q: 0.503537\n",
      " 3356/5000: episode: 748, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.277476, mean_q: 0.558566\n",
      " 3360/5000: episode: 749, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000017, mean_absolute_error: 0.256890, mean_q: 0.519479\n",
      " 3364/5000: episode: 750, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000392, mean_absolute_error: 0.207406, mean_q: 0.417385\n",
      " 3368/5000: episode: 751, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000029, mean_absolute_error: 0.247680, mean_q: 0.504127\n",
      " 3372/5000: episode: 752, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.262017, mean_q: 0.531816\n",
      " 3376/5000: episode: 753, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.239981, mean_q: 0.486072\n",
      " 3380/5000: episode: 754, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000379, mean_absolute_error: 0.236581, mean_q: 0.477793\n",
      " 3384/5000: episode: 755, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000400, mean_absolute_error: 0.247154, mean_q: 0.497243\n",
      " 3388/5000: episode: 756, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.226166, mean_q: 0.457381\n",
      " 3392/5000: episode: 757, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.240013, mean_q: 0.485147\n",
      " 3396/5000: episode: 758, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.246442, mean_q: 0.501372\n",
      " 3400/5000: episode: 759, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.222896, mean_q: 0.450689\n",
      " 3407/5000: episode: 760, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.266916, mean_q: 0.536676\n",
      " 3411/5000: episode: 761, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000389, mean_absolute_error: 0.271573, mean_q: 0.545198\n",
      " 3415/5000: episode: 762, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000018, mean_absolute_error: 0.247954, mean_q: 0.499589\n",
      " 3419/5000: episode: 763, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.229738, mean_q: 0.464430\n",
      " 3423/5000: episode: 764, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.272540, mean_q: 0.550886\n",
      " 3427/5000: episode: 765, duration: 0.018s, episode steps: 4, steps per second: 218, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.254352, mean_q: 0.512869\n",
      " 3431/5000: episode: 766, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.248874, mean_q: 0.504779\n",
      " 3436/5000: episode: 767, duration: 0.018s, episode steps: 5, steps per second: 273, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.232105, mean_q: 0.469288\n",
      " 3440/5000: episode: 768, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000006, mean_absolute_error: 0.238541, mean_q: 0.481700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3444/5000: episode: 769, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.242835, mean_q: 0.490941\n",
      " 3453/5000: episode: 770, duration: 0.034s, episode steps: 9, steps per second: 263, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000018, mean_absolute_error: 0.272618, mean_q: 0.549214\n",
      " 3458/5000: episode: 771, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.244639, mean_q: 0.493488\n",
      " 3462/5000: episode: 772, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000011, mean_absolute_error: 0.239293, mean_q: 0.483219\n",
      " 3466/5000: episode: 773, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.249261, mean_q: 0.504283\n",
      " 3470/5000: episode: 774, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.244807, mean_q: 0.493915\n",
      " 3474/5000: episode: 775, duration: 0.014s, episode steps: 4, steps per second: 284, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.250561, mean_q: 0.507883\n",
      " 3479/5000: episode: 776, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.259544, mean_q: 0.522284\n",
      " 3483/5000: episode: 777, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.283452, mean_q: 0.571174\n",
      " 3487/5000: episode: 778, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.270372, mean_q: 0.546161\n",
      " 3491/5000: episode: 779, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000789, mean_absolute_error: 0.242604, mean_q: 0.480025\n",
      " 3497/5000: episode: 780, duration: 0.024s, episode steps: 6, steps per second: 246, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.247531, mean_q: 0.497093\n",
      " 3501/5000: episode: 781, duration: 0.020s, episode steps: 4, steps per second: 200, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000038, mean_absolute_error: 0.267668, mean_q: 0.537495\n",
      " 3505/5000: episode: 782, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000851, mean_absolute_error: 0.219892, mean_q: 0.432494\n",
      " 3509/5000: episode: 783, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000063, mean_absolute_error: 0.232123, mean_q: 0.464925\n",
      " 3513/5000: episode: 784, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000063, mean_absolute_error: 0.252967, mean_q: 0.505345\n",
      " 3517/5000: episode: 785, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.271719, mean_q: 0.543901\n",
      " 3521/5000: episode: 786, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.247902, mean_q: 0.498235\n",
      " 3525/5000: episode: 787, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.227505, mean_q: 0.459439\n",
      " 3529/5000: episode: 788, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000012, mean_absolute_error: 0.274765, mean_q: 0.553174\n",
      " 3533/5000: episode: 789, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000386, mean_absolute_error: 0.253313, mean_q: 0.508854\n",
      " 3537/5000: episode: 790, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000395, mean_absolute_error: 0.258992, mean_q: 0.519564\n",
      " 3541/5000: episode: 791, duration: 0.014s, episode steps: 4, steps per second: 279, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.225176, mean_q: 0.455765\n",
      " 3545/5000: episode: 792, duration: 0.019s, episode steps: 4, steps per second: 212, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000014, mean_absolute_error: 0.275595, mean_q: 0.555150\n",
      " 3549/5000: episode: 793, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.230482, mean_q: 0.467641\n",
      " 3558/5000: episode: 794, duration: 0.032s, episode steps: 9, steps per second: 279, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000015, mean_absolute_error: 0.248218, mean_q: 0.501049\n",
      " 3563/5000: episode: 795, duration: 0.019s, episode steps: 5, steps per second: 269, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.249528, mean_q: 0.503469\n",
      " 3567/5000: episode: 796, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.282439, mean_q: 0.567934\n",
      " 3571/5000: episode: 797, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.269138, mean_q: 0.542386\n",
      " 3575/5000: episode: 798, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.230272, mean_q: 0.465439\n",
      " 3579/5000: episode: 799, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.234608, mean_q: 0.472459\n",
      " 3583/5000: episode: 800, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000003, mean_absolute_error: 0.262518, mean_q: 0.531868\n",
      " 3588/5000: episode: 801, duration: 0.019s, episode steps: 5, steps per second: 267, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000339, mean_absolute_error: 0.240437, mean_q: 0.478603\n",
      " 3592/5000: episode: 802, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000089, mean_absolute_error: 0.244192, mean_q: 0.491627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3596/5000: episode: 803, duration: 0.024s, episode steps: 4, steps per second: 169, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000030, mean_absolute_error: 0.236114, mean_q: 0.479727\n",
      " 3601/5000: episode: 804, duration: 0.018s, episode steps: 5, steps per second: 279, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.228740, mean_q: 0.464002\n",
      " 3605/5000: episode: 805, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.261868, mean_q: 0.529113\n",
      " 3609/5000: episode: 806, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000392, mean_absolute_error: 0.268051, mean_q: 0.536208\n",
      " 3613/5000: episode: 807, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000377, mean_absolute_error: 0.283967, mean_q: 0.571179\n",
      " 3617/5000: episode: 808, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000376, mean_absolute_error: 0.229490, mean_q: 0.465886\n",
      " 3621/5000: episode: 809, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000037, mean_absolute_error: 0.250480, mean_q: 0.510256\n",
      " 3626/5000: episode: 810, duration: 0.018s, episode steps: 5, steps per second: 282, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.241726, mean_q: 0.491677\n",
      " 3630/5000: episode: 811, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.251435, mean_q: 0.508292\n",
      " 3639/5000: episode: 812, duration: 0.030s, episode steps: 9, steps per second: 296, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000020, mean_absolute_error: 0.270425, mean_q: 0.544526\n",
      " 3644/5000: episode: 813, duration: 0.024s, episode steps: 5, steps per second: 208, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000308, mean_absolute_error: 0.248140, mean_q: 0.497130\n",
      " 3648/5000: episode: 814, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000020, mean_absolute_error: 0.240600, mean_q: 0.482855\n",
      " 3652/5000: episode: 815, duration: 0.023s, episode steps: 4, steps per second: 174, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000411, mean_absolute_error: 0.217614, mean_q: 0.432049\n",
      " 3657/5000: episode: 816, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000338, mean_absolute_error: 0.264873, mean_q: 0.530705\n",
      " 3661/5000: episode: 817, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000026, mean_absolute_error: 0.259526, mean_q: 0.522997\n",
      " 3665/5000: episode: 818, duration: 0.019s, episode steps: 4, steps per second: 214, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.235740, mean_q: 0.477733\n",
      " 3669/5000: episode: 819, duration: 0.018s, episode steps: 4, steps per second: 218, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000381, mean_absolute_error: 0.261243, mean_q: 0.525995\n",
      " 3673/5000: episode: 820, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.225803, mean_q: 0.456420\n",
      " 3677/5000: episode: 821, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000379, mean_absolute_error: 0.255546, mean_q: 0.513701\n",
      " 3681/5000: episode: 822, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000033, mean_absolute_error: 0.268810, mean_q: 0.541179\n",
      " 3685/5000: episode: 823, duration: 0.014s, episode steps: 4, steps per second: 284, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000023, mean_absolute_error: 0.253322, mean_q: 0.512526\n",
      " 3689/5000: episode: 824, duration: 0.014s, episode steps: 4, steps per second: 284, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.239765, mean_q: 0.484561\n",
      " 3698/5000: episode: 825, duration: 0.028s, episode steps: 9, steps per second: 317, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000019, mean_absolute_error: 0.229916, mean_q: 0.463673\n",
      " 3702/5000: episode: 826, duration: 0.019s, episode steps: 4, steps per second: 211, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000384, mean_absolute_error: 0.237171, mean_q: 0.476727\n",
      " 3706/5000: episode: 827, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000020, mean_absolute_error: 0.252695, mean_q: 0.508784\n",
      " 3710/5000: episode: 828, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000372, mean_absolute_error: 0.250996, mean_q: 0.503927\n",
      " 3714/5000: episode: 829, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000043, mean_absolute_error: 0.247728, mean_q: 0.501799\n",
      " 3718/5000: episode: 830, duration: 0.014s, episode steps: 4, steps per second: 280, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000066, mean_absolute_error: 0.262961, mean_q: 0.528729\n",
      " 3722/5000: episode: 831, duration: 0.014s, episode steps: 4, steps per second: 289, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000448, mean_absolute_error: 0.250006, mean_q: 0.502164\n",
      " 3726/5000: episode: 832, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000026, mean_absolute_error: 0.258116, mean_q: 0.522169\n",
      " 3730/5000: episode: 833, duration: 0.014s, episode steps: 4, steps per second: 286, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.256868, mean_q: 0.518745\n",
      " 3734/5000: episode: 834, duration: 0.014s, episode steps: 4, steps per second: 296, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.266252, mean_q: 0.536860\n",
      " 3741/5000: episode: 835, duration: 0.022s, episode steps: 7, steps per second: 321, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.239156, mean_q: 0.483120\n",
      " 3745/5000: episode: 836, duration: 0.014s, episode steps: 4, steps per second: 296, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000404, mean_absolute_error: 0.240957, mean_q: 0.480428\n",
      " 3749/5000: episode: 837, duration: 0.014s, episode steps: 4, steps per second: 276, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000028, mean_absolute_error: 0.233861, mean_q: 0.473064\n",
      " 3753/5000: episode: 838, duration: 0.014s, episode steps: 4, steps per second: 286, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.247522, mean_q: 0.501483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3758/5000: episode: 839, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000012, mean_absolute_error: 0.256834, mean_q: 0.521093\n",
      " 3767/5000: episode: 840, duration: 0.036s, episode steps: 9, steps per second: 251, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000025, mean_absolute_error: 0.247332, mean_q: 0.498540\n",
      " 3772/5000: episode: 841, duration: 0.018s, episode steps: 5, steps per second: 281, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.239701, mean_q: 0.481961\n",
      " 3776/5000: episode: 842, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000376, mean_absolute_error: 0.266063, mean_q: 0.532142\n",
      " 3781/5000: episode: 843, duration: 0.018s, episode steps: 5, steps per second: 283, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000022, mean_absolute_error: 0.232754, mean_q: 0.469849\n",
      " 3785/5000: episode: 844, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000032, mean_absolute_error: 0.239416, mean_q: 0.482577\n",
      " 3789/5000: episode: 845, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000045, mean_absolute_error: 0.256052, mean_q: 0.514691\n",
      " 3793/5000: episode: 846, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000036, mean_absolute_error: 0.248270, mean_q: 0.503036\n",
      " 3797/5000: episode: 847, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000374, mean_absolute_error: 0.214882, mean_q: 0.434566\n",
      " 3801/5000: episode: 848, duration: 0.014s, episode steps: 4, steps per second: 291, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.265276, mean_q: 0.538462\n",
      " 3805/5000: episode: 849, duration: 0.014s, episode steps: 4, steps per second: 292, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.223285, mean_q: 0.454014\n",
      " 3809/5000: episode: 850, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.234005, mean_q: 0.473859\n",
      " 3813/5000: episode: 851, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000005, mean_absolute_error: 0.230554, mean_q: 0.468006\n",
      " 3817/5000: episode: 852, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000373, mean_absolute_error: 0.262947, mean_q: 0.529387\n",
      " 3821/5000: episode: 853, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.246768, mean_q: 0.496889\n",
      " 3825/5000: episode: 854, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000022, mean_absolute_error: 0.253358, mean_q: 0.509948\n",
      " 3829/5000: episode: 855, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000395, mean_absolute_error: 0.239082, mean_q: 0.479402\n",
      " 3834/5000: episode: 856, duration: 0.017s, episode steps: 5, steps per second: 293, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000313, mean_absolute_error: 0.253069, mean_q: 0.509086\n",
      " 3838/5000: episode: 857, duration: 0.014s, episode steps: 4, steps per second: 283, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.235932, mean_q: 0.479027\n",
      " 3843/5000: episode: 858, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000308, mean_absolute_error: 0.276148, mean_q: 0.555157\n",
      " 3847/5000: episode: 859, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.257612, mean_q: 0.521334\n",
      " 3851/5000: episode: 860, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.274644, mean_q: 0.555227\n",
      " 3855/5000: episode: 861, duration: 0.013s, episode steps: 4, steps per second: 304, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000374, mean_absolute_error: 0.250049, mean_q: 0.504844\n",
      " 3859/5000: episode: 862, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000037, mean_absolute_error: 0.235234, mean_q: 0.474710\n",
      " 3864/5000: episode: 863, duration: 0.024s, episode steps: 5, steps per second: 209, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000360, mean_absolute_error: 0.241856, mean_q: 0.480813\n",
      " 3868/5000: episode: 864, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000492, mean_absolute_error: 0.239501, mean_q: 0.477590\n",
      " 3872/5000: episode: 865, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000050, mean_absolute_error: 0.260092, mean_q: 0.525660\n",
      " 3876/5000: episode: 866, duration: 0.014s, episode steps: 4, steps per second: 289, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000399, mean_absolute_error: 0.264434, mean_q: 0.529243\n",
      " 3880/5000: episode: 867, duration: 0.014s, episode steps: 4, steps per second: 278, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.273155, mean_q: 0.551559\n",
      " 3884/5000: episode: 868, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.258672, mean_q: 0.523417\n",
      " 3888/5000: episode: 869, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000009, mean_absolute_error: 0.246836, mean_q: 0.501284\n",
      " 3892/5000: episode: 870, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000009, mean_absolute_error: 0.275140, mean_q: 0.554862\n",
      " 3896/5000: episode: 871, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000384, mean_absolute_error: 0.265194, mean_q: 0.532547\n",
      " 3900/5000: episode: 872, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.211352, mean_q: 0.431361\n",
      " 3904/5000: episode: 873, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000037, mean_absolute_error: 0.248557, mean_q: 0.503515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3909/5000: episode: 874, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.242411, mean_q: 0.492752\n",
      " 3913/5000: episode: 875, duration: 0.019s, episode steps: 4, steps per second: 209, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.261964, mean_q: 0.533417\n",
      " 3918/5000: episode: 876, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.251641, mean_q: 0.510405\n",
      " 3922/5000: episode: 877, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000006, mean_absolute_error: 0.258043, mean_q: 0.522454\n",
      " 3931/5000: episode: 878, duration: 0.034s, episode steps: 9, steps per second: 268, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000013, mean_absolute_error: 0.246682, mean_q: 0.498795\n",
      " 3938/5000: episode: 879, duration: 0.025s, episode steps: 7, steps per second: 277, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.251962, mean_q: 0.509505\n",
      " 3943/5000: episode: 880, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000314, mean_absolute_error: 0.260366, mean_q: 0.522821\n",
      " 3947/5000: episode: 881, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.271690, mean_q: 0.548050\n",
      " 3951/5000: episode: 882, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000386, mean_absolute_error: 0.229641, mean_q: 0.459637\n",
      " 3955/5000: episode: 883, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000771, mean_absolute_error: 0.264161, mean_q: 0.527085\n",
      " 3960/5000: episode: 884, duration: 0.024s, episode steps: 5, steps per second: 207, episode reward: 1.000, mean reward: 0.200 [0.000, 0.281], mean action: 0.600 [0.000, 1.000], mean observation: 0.637 [0.256, 1.000], loss: 0.000031, mean_absolute_error: 0.244696, mean_q: 0.497588\n",
      " 3964/5000: episode: 885, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000027, mean_absolute_error: 0.263849, mean_q: 0.532051\n",
      " 3968/5000: episode: 886, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000374, mean_absolute_error: 0.240502, mean_q: 0.485662\n",
      " 3972/5000: episode: 887, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.266969, mean_q: 0.539210\n",
      " 3976/5000: episode: 888, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000365, mean_absolute_error: 0.274377, mean_q: 0.551675\n",
      " 3980/5000: episode: 889, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000387, mean_absolute_error: 0.269207, mean_q: 0.539424\n",
      " 3984/5000: episode: 890, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.274132, mean_q: 0.551928\n",
      " 3988/5000: episode: 891, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000029, mean_absolute_error: 0.211852, mean_q: 0.431293\n",
      " 3992/5000: episode: 892, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.265920, mean_q: 0.536495\n",
      " 3996/5000: episode: 893, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000388, mean_absolute_error: 0.234506, mean_q: 0.470709\n",
      " 4000/5000: episode: 894, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000378, mean_absolute_error: 0.244931, mean_q: 0.493727\n",
      " 4004/5000: episode: 895, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.237789, mean_q: 0.486850\n",
      " 4008/5000: episode: 896, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000012, mean_absolute_error: 0.253787, mean_q: 0.516654\n",
      " 4012/5000: episode: 897, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: 1.000, mean reward: 0.250 [0.039, 0.360], mean action: 0.750 [0.000, 1.000], mean observation: 0.735 [0.310, 1.000], loss: 0.000011, mean_absolute_error: 0.246684, mean_q: 0.499314\n",
      " 4021/5000: episode: 898, duration: 0.032s, episode steps: 9, steps per second: 279, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000013, mean_absolute_error: 0.243372, mean_q: 0.491432\n",
      " 4025/5000: episode: 899, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000014, mean_absolute_error: 0.279668, mean_q: 0.562900\n",
      " 4029/5000: episode: 900, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000006, mean_absolute_error: 0.251304, mean_q: 0.507174\n",
      " 4033/5000: episode: 901, duration: 0.015s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.255794, mean_q: 0.517241\n",
      " 4037/5000: episode: 902, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000379, mean_absolute_error: 0.205393, mean_q: 0.414099\n",
      " 4041/5000: episode: 903, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.262157, mean_q: 0.529396\n",
      " 4045/5000: episode: 904, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000011, mean_absolute_error: 0.264768, mean_q: 0.534527\n",
      " 4052/5000: episode: 905, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.247495, mean_q: 0.499260\n",
      " 4056/5000: episode: 906, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000009, mean_absolute_error: 0.259291, mean_q: 0.524204\n",
      " 4060/5000: episode: 907, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.285976, mean_q: 0.576920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4064/5000: episode: 908, duration: 0.018s, episode steps: 4, steps per second: 218, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000379, mean_absolute_error: 0.240591, mean_q: 0.483670\n",
      " 4068/5000: episode: 909, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000399, mean_absolute_error: 0.243341, mean_q: 0.489994\n",
      " 4072/5000: episode: 910, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000028, mean_absolute_error: 0.243235, mean_q: 0.492868\n",
      " 4076/5000: episode: 911, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000365, mean_absolute_error: 0.245374, mean_q: 0.495325\n",
      " 4080/5000: episode: 912, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000040, mean_absolute_error: 0.241489, mean_q: 0.487727\n",
      " 4084/5000: episode: 913, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.279992, mean_q: 0.566200\n",
      " 4088/5000: episode: 914, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.271299, mean_q: 0.548350\n",
      " 4092/5000: episode: 915, duration: 0.018s, episode steps: 4, steps per second: 218, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000027, mean_absolute_error: 0.241913, mean_q: 0.486313\n",
      " 4096/5000: episode: 916, duration: 0.019s, episode steps: 4, steps per second: 208, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.250815, mean_q: 0.504471\n",
      " 4100/5000: episode: 917, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.256895, mean_q: 0.517807\n",
      " 4105/5000: episode: 918, duration: 0.023s, episode steps: 5, steps per second: 215, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.258976, mean_q: 0.521398\n",
      " 4109/5000: episode: 919, duration: 0.018s, episode steps: 4, steps per second: 224, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000015, mean_absolute_error: 0.262766, mean_q: 0.530187\n",
      " 4113/5000: episode: 920, duration: 0.020s, episode steps: 4, steps per second: 204, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000384, mean_absolute_error: 0.227879, mean_q: 0.457981\n",
      " 4118/5000: episode: 921, duration: 0.021s, episode steps: 5, steps per second: 240, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000019, mean_absolute_error: 0.271325, mean_q: 0.549655\n",
      " 4122/5000: episode: 922, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000380, mean_absolute_error: 0.254477, mean_q: 0.514745\n",
      " 4127/5000: episode: 923, duration: 0.023s, episode steps: 5, steps per second: 216, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000035, mean_absolute_error: 0.230518, mean_q: 0.468717\n",
      " 4131/5000: episode: 924, duration: 0.019s, episode steps: 4, steps per second: 208, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.248482, mean_q: 0.503136\n",
      " 4135/5000: episode: 925, duration: 0.019s, episode steps: 4, steps per second: 216, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000029, mean_absolute_error: 0.234739, mean_q: 0.474989\n",
      " 4139/5000: episode: 926, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.256716, mean_q: 0.517892\n",
      " 4145/5000: episode: 927, duration: 0.024s, episode steps: 6, steps per second: 250, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.263296, mean_q: 0.530945\n",
      " 4149/5000: episode: 928, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.256281, mean_q: 0.517672\n",
      " 4153/5000: episode: 929, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000026, mean_absolute_error: 0.273859, mean_q: 0.552087\n",
      " 4160/5000: episode: 930, duration: 0.028s, episode steps: 7, steps per second: 248, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000222, mean_absolute_error: 0.241802, mean_q: 0.485989\n",
      " 4164/5000: episode: 931, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.255373, mean_q: 0.515125\n",
      " 4168/5000: episode: 932, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000018, mean_absolute_error: 0.271267, mean_q: 0.549379\n",
      " 4172/5000: episode: 933, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000012, mean_absolute_error: 0.256806, mean_q: 0.521921\n",
      " 4176/5000: episode: 934, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000400, mean_absolute_error: 0.252895, mean_q: 0.507991\n",
      " 4180/5000: episode: 935, duration: 0.018s, episode steps: 4, steps per second: 226, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.253696, mean_q: 0.513259\n",
      " 4184/5000: episode: 936, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000018, mean_absolute_error: 0.242293, mean_q: 0.490420\n",
      " 4188/5000: episode: 937, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.220505, mean_q: 0.446760\n",
      " 4192/5000: episode: 938, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000028, mean_absolute_error: 0.255698, mean_q: 0.515129\n",
      " 4196/5000: episode: 939, duration: 0.016s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000036, mean_absolute_error: 0.260470, mean_q: 0.520787\n",
      " 4200/5000: episode: 940, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.288586, mean_q: 0.581210\n",
      " 4204/5000: episode: 941, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000031, mean_absolute_error: 0.237506, mean_q: 0.476563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4208/5000: episode: 942, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.230656, mean_q: 0.465904\n",
      " 4212/5000: episode: 943, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.269961, mean_q: 0.545483\n",
      " 4216/5000: episode: 944, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.238856, mean_q: 0.480584\n",
      " 4220/5000: episode: 945, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.248951, mean_q: 0.500788\n",
      " 4224/5000: episode: 946, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.268696, mean_q: 0.543177\n",
      " 4229/5000: episode: 947, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.251502, mean_q: 0.508039\n",
      " 4233/5000: episode: 948, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.262965, mean_q: 0.531277\n",
      " 4242/5000: episode: 949, duration: 0.032s, episode steps: 9, steps per second: 281, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000191, mean_absolute_error: 0.263521, mean_q: 0.528153\n",
      " 4246/5000: episode: 950, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.240430, mean_q: 0.482713\n",
      " 4250/5000: episode: 951, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000011, mean_absolute_error: 0.254864, mean_q: 0.514311\n",
      " 4254/5000: episode: 952, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000013, mean_absolute_error: 0.246256, mean_q: 0.498684\n",
      " 4258/5000: episode: 953, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.263966, mean_q: 0.533554\n",
      " 4263/5000: episode: 954, duration: 0.024s, episode steps: 5, steps per second: 206, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000350, mean_absolute_error: 0.254586, mean_q: 0.507777\n",
      " 4267/5000: episode: 955, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000017, mean_absolute_error: 0.253165, mean_q: 0.511441\n",
      " 4271/5000: episode: 956, duration: 0.020s, episode steps: 4, steps per second: 201, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.238544, mean_q: 0.479798\n",
      " 4275/5000: episode: 957, duration: 0.018s, episode steps: 4, steps per second: 218, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.264872, mean_q: 0.533883\n",
      " 4284/5000: episode: 958, duration: 0.041s, episode steps: 9, steps per second: 218, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000006, mean_absolute_error: 0.249126, mean_q: 0.502419\n",
      " 4293/5000: episode: 959, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000185, mean_absolute_error: 0.231376, mean_q: 0.464643\n",
      " 4298/5000: episode: 960, duration: 0.021s, episode steps: 5, steps per second: 236, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.235401, mean_q: 0.474683\n",
      " 4302/5000: episode: 961, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.273742, mean_q: 0.551269\n",
      " 4306/5000: episode: 962, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.236656, mean_q: 0.479291\n",
      " 4310/5000: episode: 963, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.261293, mean_q: 0.528715\n",
      " 4314/5000: episode: 964, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000005, mean_absolute_error: 0.249220, mean_q: 0.503262\n",
      " 4318/5000: episode: 965, duration: 0.015s, episode steps: 4, steps per second: 273, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.256374, mean_q: 0.517528\n",
      " 4322/5000: episode: 966, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.242659, mean_q: 0.490167\n",
      " 4326/5000: episode: 967, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000006, mean_absolute_error: 0.249824, mean_q: 0.505169\n",
      " 4330/5000: episode: 968, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000417, mean_absolute_error: 0.262884, mean_q: 0.521848\n",
      " 4334/5000: episode: 969, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000419, mean_absolute_error: 0.261992, mean_q: 0.524610\n",
      " 4338/5000: episode: 970, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000031, mean_absolute_error: 0.242238, mean_q: 0.489375\n",
      " 4342/5000: episode: 971, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.249784, mean_q: 0.504660\n",
      " 4346/5000: episode: 972, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.240236, mean_q: 0.486038\n",
      " 4351/5000: episode: 973, duration: 0.019s, episode steps: 5, steps per second: 265, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000310, mean_absolute_error: 0.239174, mean_q: 0.479952\n",
      " 4355/5000: episode: 974, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000005, mean_absolute_error: 0.236240, mean_q: 0.478247\n",
      " 4359/5000: episode: 975, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000009, mean_absolute_error: 0.264171, mean_q: 0.533380\n",
      " 4363/5000: episode: 976, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.257128, mean_q: 0.519666\n",
      " 4367/5000: episode: 977, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.251794, mean_q: 0.509400\n",
      " 4371/5000: episode: 978, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000010, mean_absolute_error: 0.224264, mean_q: 0.453919\n",
      " 4375/5000: episode: 979, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.236800, mean_q: 0.478617\n",
      " 4379/5000: episode: 980, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.250289, mean_q: 0.503968\n",
      " 4383/5000: episode: 981, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000004, mean_absolute_error: 0.256704, mean_q: 0.518400\n",
      " 4387/5000: episode: 982, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000003, mean_absolute_error: 0.252917, mean_q: 0.511499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4393/5000: episode: 983, duration: 0.027s, episode steps: 6, steps per second: 220, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000273, mean_absolute_error: 0.251505, mean_q: 0.504459\n",
      " 4397/5000: episode: 984, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.247795, mean_q: 0.498670\n",
      " 4401/5000: episode: 985, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000019, mean_absolute_error: 0.255454, mean_q: 0.513340\n",
      " 4405/5000: episode: 986, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000036, mean_absolute_error: 0.234927, mean_q: 0.471630\n",
      " 4409/5000: episode: 987, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000388, mean_absolute_error: 0.244148, mean_q: 0.490444\n",
      " 4413/5000: episode: 988, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.243746, mean_q: 0.493458\n",
      " 4418/5000: episode: 989, duration: 0.018s, episode steps: 5, steps per second: 272, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.244736, mean_q: 0.494268\n",
      " 4422/5000: episode: 990, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.269782, mean_q: 0.545275\n",
      " 4426/5000: episode: 991, duration: 0.014s, episode steps: 4, steps per second: 283, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000394, mean_absolute_error: 0.250980, mean_q: 0.503259\n",
      " 4430/5000: episode: 992, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.247912, mean_q: 0.498042\n",
      " 4435/5000: episode: 993, duration: 0.018s, episode steps: 5, steps per second: 284, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000304, mean_absolute_error: 0.268886, mean_q: 0.539804\n",
      " 4439/5000: episode: 994, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000381, mean_absolute_error: 0.230415, mean_q: 0.465643\n",
      " 4443/5000: episode: 995, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000014, mean_absolute_error: 0.266576, mean_q: 0.539615\n",
      " 4447/5000: episode: 996, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000011, mean_absolute_error: 0.250423, mean_q: 0.507976\n",
      " 4451/5000: episode: 997, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.254383, mean_q: 0.513274\n",
      " 4455/5000: episode: 998, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000022, mean_absolute_error: 0.224085, mean_q: 0.451544\n",
      " 4459/5000: episode: 999, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.231464, mean_q: 0.467497\n",
      " 4463/5000: episode: 1000, duration: 0.015s, episode steps: 4, steps per second: 269, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000391, mean_absolute_error: 0.218744, mean_q: 0.441066\n",
      " 4467/5000: episode: 1001, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000388, mean_absolute_error: 0.262555, mean_q: 0.527236\n",
      " 4471/5000: episode: 1002, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.237437, mean_q: 0.483400\n",
      " 4475/5000: episode: 1003, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.234709, mean_q: 0.475434\n",
      " 4479/5000: episode: 1004, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.246577, mean_q: 0.499346\n",
      " 4486/5000: episode: 1005, duration: 0.026s, episode steps: 7, steps per second: 274, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000229, mean_absolute_error: 0.261006, mean_q: 0.524400\n",
      " 4490/5000: episode: 1006, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000033, mean_absolute_error: 0.258246, mean_q: 0.518333\n",
      " 4495/5000: episode: 1007, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: 1.000, mean reward: 0.200 [0.038, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.696 [0.310, 1.000], loss: 0.000029, mean_absolute_error: 0.254745, mean_q: 0.513321\n",
      " 4499/5000: episode: 1008, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.249771, mean_q: 0.507864\n",
      " 4503/5000: episode: 1009, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000004, mean_absolute_error: 0.275959, mean_q: 0.557915\n",
      " 4507/5000: episode: 1010, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000014, mean_absolute_error: 0.283064, mean_q: 0.569112\n",
      " 4513/5000: episode: 1011, duration: 0.021s, episode steps: 6, steps per second: 285, episode reward: 1.000, mean reward: 0.167 [0.000, 0.414], mean action: 0.333 [0.000, 1.000], mean observation: 0.802 [0.256, 1.000], loss: 0.000267, mean_absolute_error: 0.244986, mean_q: 0.490805\n",
      " 4517/5000: episode: 1012, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000023, mean_absolute_error: 0.256922, mean_q: 0.518614\n",
      " 4521/5000: episode: 1013, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000011, mean_absolute_error: 0.274261, mean_q: 0.553961\n",
      " 4525/5000: episode: 1014, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.245904, mean_q: 0.497220\n",
      " 4529/5000: episode: 1015, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.249287, mean_q: 0.503785\n",
      " 4533/5000: episode: 1016, duration: 0.015s, episode steps: 4, steps per second: 276, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.230602, mean_q: 0.465868\n",
      " 4537/5000: episode: 1017, duration: 0.015s, episode steps: 4, steps per second: 275, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000744, mean_absolute_error: 0.255419, mean_q: 0.507998\n",
      " 4541/5000: episode: 1018, duration: 0.014s, episode steps: 4, steps per second: 277, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000401, mean_absolute_error: 0.257870, mean_q: 0.518841\n",
      " 4545/5000: episode: 1019, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.039, 0.360], mean action: 0.750 [0.000, 1.000], mean observation: 0.735 [0.310, 1.000], loss: 0.000045, mean_absolute_error: 0.244896, mean_q: 0.497727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4549/5000: episode: 1020, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.269952, mean_q: 0.548273\n",
      " 4553/5000: episode: 1021, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000029, mean_absolute_error: 0.265452, mean_q: 0.537593\n",
      " 4557/5000: episode: 1022, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000026, mean_absolute_error: 0.230323, mean_q: 0.464719\n",
      " 4564/5000: episode: 1023, duration: 0.025s, episode steps: 7, steps per second: 277, episode reward: 1.000, mean reward: 0.143 [0.000, 0.360], mean action: 0.429 [0.000, 1.000], mean observation: 0.832 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.269831, mean_q: 0.541710\n",
      " 4568/5000: episode: 1024, duration: 0.015s, episode steps: 4, steps per second: 266, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000027, mean_absolute_error: 0.245175, mean_q: 0.494202\n",
      " 4572/5000: episode: 1025, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.283390, mean_q: 0.569130\n",
      " 4576/5000: episode: 1026, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.255388, mean_q: 0.514397\n",
      " 4580/5000: episode: 1027, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000392, mean_absolute_error: 0.228099, mean_q: 0.457443\n",
      " 4584/5000: episode: 1028, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.250800, mean_q: 0.506813\n",
      " 4588/5000: episode: 1029, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000023, mean_absolute_error: 0.236651, mean_q: 0.478876\n",
      " 4592/5000: episode: 1030, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.264227, mean_q: 0.531729\n",
      " 4596/5000: episode: 1031, duration: 0.015s, episode steps: 4, steps per second: 271, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.267823, mean_q: 0.540580\n",
      " 4600/5000: episode: 1032, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000792, mean_absolute_error: 0.246467, mean_q: 0.487460\n",
      " 4604/5000: episode: 1033, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.001154, mean_absolute_error: 0.262982, mean_q: 0.522393\n",
      " 4608/5000: episode: 1034, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000138, mean_absolute_error: 0.231979, mean_q: 0.467233\n",
      " 4613/5000: episode: 1035, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: 1.000, mean reward: 0.200 [0.039, 0.291], mean action: 0.600 [0.000, 1.000], mean observation: 0.675 [0.256, 1.000], loss: 0.000097, mean_absolute_error: 0.262605, mean_q: 0.529928\n",
      " 4618/5000: episode: 1036, duration: 0.019s, episode steps: 5, steps per second: 265, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000048, mean_absolute_error: 0.274798, mean_q: 0.554196\n",
      " 4627/5000: episode: 1037, duration: 0.031s, episode steps: 9, steps per second: 288, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000358, mean_absolute_error: 0.254435, mean_q: 0.506130\n",
      " 4631/5000: episode: 1038, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000041, mean_absolute_error: 0.251032, mean_q: 0.503208\n",
      " 4635/5000: episode: 1039, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000033, mean_absolute_error: 0.269628, mean_q: 0.545173\n",
      " 4639/5000: episode: 1040, duration: 0.016s, episode steps: 4, steps per second: 254, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000021, mean_absolute_error: 0.256442, mean_q: 0.518164\n",
      " 4643/5000: episode: 1041, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.228957, mean_q: 0.465083\n",
      " 4647/5000: episode: 1042, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.257060, mean_q: 0.519847\n",
      " 4651/5000: episode: 1043, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.254465, mean_q: 0.514540\n",
      " 4655/5000: episode: 1044, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000008, mean_absolute_error: 0.253617, mean_q: 0.513575\n",
      " 4659/5000: episode: 1045, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.245526, mean_q: 0.494761\n",
      " 4668/5000: episode: 1046, duration: 0.036s, episode steps: 9, steps per second: 253, episode reward: 1.000, mean reward: 0.111 [0.000, 0.414], mean action: 0.222 [0.000, 1.000], mean observation: 0.856 [0.256, 1.000], loss: 0.000352, mean_absolute_error: 0.236418, mean_q: 0.472902\n",
      " 4672/5000: episode: 1047, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000429, mean_absolute_error: 0.260854, mean_q: 0.521384\n",
      " 4676/5000: episode: 1048, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000077, mean_absolute_error: 0.240879, mean_q: 0.484089\n",
      " 4680/5000: episode: 1049, duration: 0.016s, episode steps: 4, steps per second: 256, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000053, mean_absolute_error: 0.226931, mean_q: 0.459951\n",
      " 4684/5000: episode: 1050, duration: 0.015s, episode steps: 4, steps per second: 258, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.258444, mean_q: 0.524135\n",
      " 4688/5000: episode: 1051, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.270204, mean_q: 0.547585\n",
      " 4692/5000: episode: 1052, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.265245, mean_q: 0.534966\n",
      " 4696/5000: episode: 1053, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000017, mean_absolute_error: 0.254404, mean_q: 0.512365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4701/5000: episode: 1054, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.253445, mean_q: 0.511026\n",
      " 4705/5000: episode: 1055, duration: 0.020s, episode steps: 4, steps per second: 203, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.255842, mean_q: 0.517413\n",
      " 4709/5000: episode: 1056, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000394, mean_absolute_error: 0.235925, mean_q: 0.472936\n",
      " 4713/5000: episode: 1057, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000386, mean_absolute_error: 0.242363, mean_q: 0.488240\n",
      " 4717/5000: episode: 1058, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000014, mean_absolute_error: 0.243831, mean_q: 0.496094\n",
      " 4721/5000: episode: 1059, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.039, 0.360], mean action: 0.750 [0.000, 1.000], mean observation: 0.735 [0.310, 1.000], loss: 0.000012, mean_absolute_error: 0.272021, mean_q: 0.551401\n",
      " 4725/5000: episode: 1060, duration: 0.015s, episode steps: 4, steps per second: 272, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000018, mean_absolute_error: 0.261947, mean_q: 0.531634\n",
      " 4729/5000: episode: 1061, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000020, mean_absolute_error: 0.257004, mean_q: 0.519444\n",
      " 4733/5000: episode: 1062, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.222781, mean_q: 0.452817\n",
      " 4742/5000: episode: 1063, duration: 0.029s, episode steps: 9, steps per second: 310, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000016, mean_absolute_error: 0.254705, mean_q: 0.513567\n",
      " 4746/5000: episode: 1064, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000007, mean_absolute_error: 0.257808, mean_q: 0.519658\n",
      " 4751/5000: episode: 1065, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.250876, mean_q: 0.505597\n",
      " 4755/5000: episode: 1066, duration: 0.018s, episode steps: 4, steps per second: 217, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.231303, mean_q: 0.469875\n",
      " 4759/5000: episode: 1067, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.272270, mean_q: 0.549587\n",
      " 4766/5000: episode: 1068, duration: 0.026s, episode steps: 7, steps per second: 274, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000017, mean_absolute_error: 0.235474, mean_q: 0.475270\n",
      " 4770/5000: episode: 1069, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000006, mean_absolute_error: 0.245837, mean_q: 0.496855\n",
      " 4774/5000: episode: 1070, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.244997, mean_q: 0.494923\n",
      " 4778/5000: episode: 1071, duration: 0.016s, episode steps: 4, steps per second: 257, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.246634, mean_q: 0.499156\n",
      " 4782/5000: episode: 1072, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000016, mean_absolute_error: 0.256334, mean_q: 0.517001\n",
      " 4786/5000: episode: 1073, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000006, mean_absolute_error: 0.255430, mean_q: 0.516004\n",
      " 4791/5000: episode: 1074, duration: 0.018s, episode steps: 5, steps per second: 271, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000007, mean_absolute_error: 0.260534, mean_q: 0.525390\n",
      " 4795/5000: episode: 1075, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.252939, mean_q: 0.511299\n",
      " 4803/5000: episode: 1076, duration: 0.033s, episode steps: 8, steps per second: 242, episode reward: 1.000, mean reward: 0.125 [0.000, 0.414], mean action: 0.250 [0.000, 1.000], mean observation: 0.842 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.254875, mean_q: 0.515260\n",
      " 4807/5000: episode: 1077, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.243629, mean_q: 0.490205\n",
      " 4811/5000: episode: 1078, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000022, mean_absolute_error: 0.263921, mean_q: 0.531644\n",
      " 4815/5000: episode: 1079, duration: 0.020s, episode steps: 4, steps per second: 205, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000390, mean_absolute_error: 0.251390, mean_q: 0.505145\n",
      " 4819/5000: episode: 1080, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000027, mean_absolute_error: 0.256744, mean_q: 0.518603\n",
      " 4823/5000: episode: 1081, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.233852, mean_q: 0.474465\n",
      " 4827/5000: episode: 1082, duration: 0.015s, episode steps: 4, steps per second: 265, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.230105, mean_q: 0.466767\n",
      " 4831/5000: episode: 1083, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.233577, mean_q: 0.472436\n",
      " 4838/5000: episode: 1084, duration: 0.025s, episode steps: 7, steps per second: 281, episode reward: 1.000, mean reward: 0.143 [0.000, 0.414], mean action: 0.286 [0.000, 1.000], mean observation: 0.825 [0.256, 1.000], loss: 0.000238, mean_absolute_error: 0.226785, mean_q: 0.454477\n",
      " 4842/5000: episode: 1085, duration: 0.015s, episode steps: 4, steps per second: 263, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000016, mean_absolute_error: 0.261963, mean_q: 0.526435\n",
      " 4846/5000: episode: 1086, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000021, mean_absolute_error: 0.264784, mean_q: 0.533641\n",
      " 4850/5000: episode: 1087, duration: 0.016s, episode steps: 4, steps per second: 255, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000024, mean_absolute_error: 0.249493, mean_q: 0.502386\n",
      " 4854/5000: episode: 1088, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000015, mean_absolute_error: 0.235063, mean_q: 0.477555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4858/5000: episode: 1089, duration: 0.015s, episode steps: 4, steps per second: 259, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000004, mean_absolute_error: 0.240577, mean_q: 0.487825\n",
      " 4863/5000: episode: 1090, duration: 0.022s, episode steps: 5, steps per second: 231, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000010, mean_absolute_error: 0.238861, mean_q: 0.483394\n",
      " 4868/5000: episode: 1091, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.265826, mean_q: 0.536417\n",
      " 4872/5000: episode: 1092, duration: 0.015s, episode steps: 4, steps per second: 264, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000014, mean_absolute_error: 0.261398, mean_q: 0.525223\n",
      " 4876/5000: episode: 1093, duration: 0.015s, episode steps: 4, steps per second: 270, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000013, mean_absolute_error: 0.223754, mean_q: 0.450931\n",
      " 4880/5000: episode: 1094, duration: 0.015s, episode steps: 4, steps per second: 268, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000025, mean_absolute_error: 0.212821, mean_q: 0.429157\n",
      " 4884/5000: episode: 1095, duration: 0.015s, episode steps: 4, steps per second: 262, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000003, mean_absolute_error: 0.260219, mean_q: 0.527780\n",
      " 4888/5000: episode: 1096, duration: 0.015s, episode steps: 4, steps per second: 267, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.234071, mean_q: 0.474608\n",
      " 4892/5000: episode: 1097, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000010, mean_absolute_error: 0.238611, mean_q: 0.481855\n",
      " 4901/5000: episode: 1098, duration: 0.032s, episode steps: 9, steps per second: 281, episode reward: 0.962, mean reward: 0.107 [0.000, 0.414], mean action: 0.111 [0.000, 1.000], mean observation: 0.851 [0.256, 0.962], loss: 0.000007, mean_absolute_error: 0.266145, mean_q: 0.536400\n",
      " 4905/5000: episode: 1099, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000007, mean_absolute_error: 0.252425, mean_q: 0.509454\n",
      " 4909/5000: episode: 1100, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000011, mean_absolute_error: 0.222699, mean_q: 0.450198\n",
      " 4913/5000: episode: 1101, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.218776, mean_q: 0.444765\n",
      " 4917/5000: episode: 1102, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000006, mean_absolute_error: 0.234815, mean_q: 0.475189\n",
      " 4921/5000: episode: 1103, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000003, mean_absolute_error: 0.244160, mean_q: 0.494436\n",
      " 4926/5000: episode: 1104, duration: 0.021s, episode steps: 5, steps per second: 241, episode reward: 1.000, mean reward: 0.200 [0.000, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.770 [0.256, 1.000], loss: 0.000320, mean_absolute_error: 0.266530, mean_q: 0.534811\n",
      " 4931/5000: episode: 1105, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000318, mean_absolute_error: 0.267840, mean_q: 0.536556\n",
      " 4935/5000: episode: 1106, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000019, mean_absolute_error: 0.250327, mean_q: 0.505700\n",
      " 4939/5000: episode: 1107, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000012, mean_absolute_error: 0.269795, mean_q: 0.546003\n",
      " 4944/5000: episode: 1108, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: 1.000, mean reward: 0.200 [0.000, 0.310], mean action: 0.800 [0.000, 1.000], mean observation: 0.679 [0.310, 1.000], loss: 0.000013, mean_absolute_error: 0.227918, mean_q: 0.461630\n",
      " 4948/5000: episode: 1109, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000009, mean_absolute_error: 0.228438, mean_q: 0.461018\n",
      " 4952/5000: episode: 1110, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.243933, mean_q: 0.491561\n",
      " 4957/5000: episode: 1111, duration: 0.023s, episode steps: 5, steps per second: 222, episode reward: 1.000, mean reward: 0.200 [0.002, 0.414], mean action: 0.400 [0.000, 1.000], mean observation: 0.713 [0.256, 1.000], loss: 0.000008, mean_absolute_error: 0.273005, mean_q: 0.549420\n",
      " 4961/5000: episode: 1112, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000388, mean_absolute_error: 0.245160, mean_q: 0.491073\n",
      " 4965/5000: episode: 1113, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000019, mean_absolute_error: 0.265030, mean_q: 0.535315\n",
      " 4969/5000: episode: 1114, duration: 0.015s, episode steps: 4, steps per second: 260, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000777, mean_absolute_error: 0.265737, mean_q: 0.524631\n",
      " 4973/5000: episode: 1115, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000130, mean_absolute_error: 0.242301, mean_q: 0.488331\n",
      " 4977/5000: episode: 1116, duration: 0.020s, episode steps: 4, steps per second: 203, episode reward: 1.000, mean reward: 0.250 [0.230, 0.281], mean action: 0.750 [0.000, 1.000], mean observation: 0.616 [0.256, 1.000], loss: 0.000107, mean_absolute_error: 0.242467, mean_q: 0.486900\n",
      " 4981/5000: episode: 1117, duration: 0.019s, episode steps: 4, steps per second: 206, episode reward: 1.000, mean reward: 0.250 [0.038, 0.414], mean action: 0.500 [0.000, 1.000], mean observation: 0.722 [0.256, 1.000], loss: 0.000048, mean_absolute_error: 0.259677, mean_q: 0.524834\n",
      " 4985/5000: episode: 1118, duration: 0.019s, episode steps: 4, steps per second: 206, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000034, mean_absolute_error: 0.239228, mean_q: 0.480130\n",
      " 4989/5000: episode: 1119, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000022, mean_absolute_error: 0.242167, mean_q: 0.484143\n",
      " 4994/5000: episode: 1120, duration: 0.022s, episode steps: 5, steps per second: 232, episode reward: 1.000, mean reward: 0.200 [0.000, 0.360], mean action: 0.600 [0.000, 1.000], mean observation: 0.780 [0.310, 1.000], loss: 0.000038, mean_absolute_error: 0.256917, mean_q: 0.512681\n",
      " 4998/5000: episode: 1121, duration: 0.019s, episode steps: 4, steps per second: 208, episode reward: 1.000, mean reward: 0.250 [0.225, 0.310], mean action: 1.000 [1.000, 1.000], mean observation: 0.655 [0.310, 1.000], loss: 0.000029, mean_absolute_error: 0.264522, mean_q: 0.530083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 21.679 seconds\n"
     ]
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "hist = dqn.fit(env, nb_steps=5000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['episode_reward', 'nb_episode_steps', 'nb_steps'])\n"
     ]
    }
   ],
   "source": [
    "print(hist.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 18,\n",
       " 27,\n",
       " 36,\n",
       " 45,\n",
       " 54,\n",
       " 63,\n",
       " 67,\n",
       " 71,\n",
       " 75,\n",
       " 79,\n",
       " 83,\n",
       " 87,\n",
       " 91,\n",
       " 95,\n",
       " 99,\n",
       " 103,\n",
       " 107,\n",
       " 111,\n",
       " 115,\n",
       " 119,\n",
       " 123,\n",
       " 127,\n",
       " 131,\n",
       " 135,\n",
       " 139,\n",
       " 143,\n",
       " 147,\n",
       " 151,\n",
       " 155,\n",
       " 159,\n",
       " 163,\n",
       " 167,\n",
       " 171,\n",
       " 175,\n",
       " 179,\n",
       " 183,\n",
       " 187,\n",
       " 192,\n",
       " 196,\n",
       " 200,\n",
       " 204,\n",
       " 209,\n",
       " 213,\n",
       " 217,\n",
       " 222,\n",
       " 226,\n",
       " 230,\n",
       " 234,\n",
       " 238,\n",
       " 242,\n",
       " 246,\n",
       " 250,\n",
       " 255,\n",
       " 259,\n",
       " 263,\n",
       " 267,\n",
       " 271,\n",
       " 275,\n",
       " 279,\n",
       " 283,\n",
       " 287,\n",
       " 291,\n",
       " 295,\n",
       " 299,\n",
       " 303,\n",
       " 307,\n",
       " 311,\n",
       " 315,\n",
       " 319,\n",
       " 323,\n",
       " 327,\n",
       " 331,\n",
       " 335,\n",
       " 339,\n",
       " 343,\n",
       " 347,\n",
       " 351,\n",
       " 355,\n",
       " 359,\n",
       " 363,\n",
       " 367,\n",
       " 371,\n",
       " 376,\n",
       " 380,\n",
       " 384,\n",
       " 388,\n",
       " 392,\n",
       " 396,\n",
       " 400,\n",
       " 404,\n",
       " 408,\n",
       " 412,\n",
       " 416,\n",
       " 420,\n",
       " 424,\n",
       " 428,\n",
       " 432,\n",
       " 436,\n",
       " 440,\n",
       " 444,\n",
       " 448,\n",
       " 452,\n",
       " 456,\n",
       " 460,\n",
       " 464,\n",
       " 468,\n",
       " 472,\n",
       " 476,\n",
       " 480,\n",
       " 484,\n",
       " 488,\n",
       " 492,\n",
       " 496,\n",
       " 500,\n",
       " 504,\n",
       " 508,\n",
       " 512,\n",
       " 516,\n",
       " 520,\n",
       " 524,\n",
       " 528,\n",
       " 532,\n",
       " 536,\n",
       " 540,\n",
       " 544,\n",
       " 548,\n",
       " 552,\n",
       " 556,\n",
       " 560,\n",
       " 564,\n",
       " 568,\n",
       " 574,\n",
       " 578,\n",
       " 583,\n",
       " 587,\n",
       " 591,\n",
       " 595,\n",
       " 599,\n",
       " 603,\n",
       " 608,\n",
       " 612,\n",
       " 616,\n",
       " 620,\n",
       " 624,\n",
       " 633,\n",
       " 638,\n",
       " 642,\n",
       " 646,\n",
       " 650,\n",
       " 654,\n",
       " 658,\n",
       " 662,\n",
       " 667,\n",
       " 671,\n",
       " 675,\n",
       " 679,\n",
       " 683,\n",
       " 687,\n",
       " 691,\n",
       " 696,\n",
       " 705,\n",
       " 709,\n",
       " 718,\n",
       " 722,\n",
       " 726,\n",
       " 730,\n",
       " 734,\n",
       " 739,\n",
       " 743,\n",
       " 748,\n",
       " 754,\n",
       " 758,\n",
       " 762,\n",
       " 766,\n",
       " 770,\n",
       " 774,\n",
       " 778,\n",
       " 782,\n",
       " 786,\n",
       " 790,\n",
       " 794,\n",
       " 798,\n",
       " 802,\n",
       " 806,\n",
       " 810,\n",
       " 819,\n",
       " 823,\n",
       " 827,\n",
       " 831,\n",
       " 835,\n",
       " 839,\n",
       " 844,\n",
       " 848,\n",
       " 852,\n",
       " 856,\n",
       " 860,\n",
       " 864,\n",
       " 869,\n",
       " 873,\n",
       " 877,\n",
       " 881,\n",
       " 885,\n",
       " 889,\n",
       " 893,\n",
       " 897,\n",
       " 901,\n",
       " 905,\n",
       " 909,\n",
       " 913,\n",
       " 917,\n",
       " 921,\n",
       " 925,\n",
       " 929,\n",
       " 933,\n",
       " 937,\n",
       " 941,\n",
       " 946,\n",
       " 950,\n",
       " 954,\n",
       " 958,\n",
       " 962,\n",
       " 966,\n",
       " 970,\n",
       " 974,\n",
       " 978,\n",
       " 982,\n",
       " 986,\n",
       " 990,\n",
       " 997,\n",
       " 1001,\n",
       " 1005,\n",
       " 1009,\n",
       " 1013,\n",
       " 1017,\n",
       " 1021,\n",
       " 1025,\n",
       " 1029,\n",
       " 1033,\n",
       " 1037,\n",
       " 1041,\n",
       " 1050,\n",
       " 1059,\n",
       " 1064,\n",
       " 1073,\n",
       " 1077,\n",
       " 1081,\n",
       " 1085,\n",
       " 1089,\n",
       " 1093,\n",
       " 1097,\n",
       " 1101,\n",
       " 1105,\n",
       " 1109,\n",
       " 1114,\n",
       " 1123,\n",
       " 1131,\n",
       " 1135,\n",
       " 1139,\n",
       " 1143,\n",
       " 1148,\n",
       " 1153,\n",
       " 1162,\n",
       " 1166,\n",
       " 1170,\n",
       " 1179,\n",
       " 1183,\n",
       " 1188,\n",
       " 1192,\n",
       " 1196,\n",
       " 1200,\n",
       " 1204,\n",
       " 1208,\n",
       " 1212,\n",
       " 1217,\n",
       " 1226,\n",
       " 1230,\n",
       " 1234,\n",
       " 1238,\n",
       " 1242,\n",
       " 1246,\n",
       " 1250,\n",
       " 1254,\n",
       " 1258,\n",
       " 1262,\n",
       " 1267,\n",
       " 1272,\n",
       " 1276,\n",
       " 1285,\n",
       " 1294,\n",
       " 1301,\n",
       " 1305,\n",
       " 1309,\n",
       " 1315,\n",
       " 1320,\n",
       " 1324,\n",
       " 1328,\n",
       " 1333,\n",
       " 1337,\n",
       " 1341,\n",
       " 1345,\n",
       " 1349,\n",
       " 1353,\n",
       " 1357,\n",
       " 1361,\n",
       " 1365,\n",
       " 1369,\n",
       " 1373,\n",
       " 1377,\n",
       " 1381,\n",
       " 1385,\n",
       " 1389,\n",
       " 1393,\n",
       " 1397,\n",
       " 1402,\n",
       " 1406,\n",
       " 1410,\n",
       " 1414,\n",
       " 1418,\n",
       " 1425,\n",
       " 1429,\n",
       " 1433,\n",
       " 1437,\n",
       " 1441,\n",
       " 1450,\n",
       " 1454,\n",
       " 1458,\n",
       " 1462,\n",
       " 1466,\n",
       " 1473,\n",
       " 1477,\n",
       " 1481,\n",
       " 1490,\n",
       " 1494,\n",
       " 1500,\n",
       " 1504,\n",
       " 1508,\n",
       " 1512,\n",
       " 1516,\n",
       " 1520,\n",
       " 1529,\n",
       " 1533,\n",
       " 1537,\n",
       " 1541,\n",
       " 1545,\n",
       " 1549,\n",
       " 1553,\n",
       " 1557,\n",
       " 1561,\n",
       " 1570,\n",
       " 1579,\n",
       " 1583,\n",
       " 1587,\n",
       " 1596,\n",
       " 1601,\n",
       " 1605,\n",
       " 1609,\n",
       " 1613,\n",
       " 1617,\n",
       " 1621,\n",
       " 1625,\n",
       " 1629,\n",
       " 1633,\n",
       " 1637,\n",
       " 1642,\n",
       " 1649,\n",
       " 1653,\n",
       " 1657,\n",
       " 1661,\n",
       " 1665,\n",
       " 1674,\n",
       " 1678,\n",
       " 1682,\n",
       " 1687,\n",
       " 1692,\n",
       " 1696,\n",
       " 1701,\n",
       " 1705,\n",
       " 1714,\n",
       " 1719,\n",
       " 1724,\n",
       " 1728,\n",
       " 1732,\n",
       " 1736,\n",
       " 1740,\n",
       " 1744,\n",
       " 1748,\n",
       " 1753,\n",
       " 1757,\n",
       " 1761,\n",
       " 1765,\n",
       " 1769,\n",
       " 1773,\n",
       " 1777,\n",
       " 1781,\n",
       " 1785,\n",
       " 1794,\n",
       " 1798,\n",
       " 1802,\n",
       " 1806,\n",
       " 1810,\n",
       " 1814,\n",
       " 1818,\n",
       " 1822,\n",
       " 1827,\n",
       " 1831,\n",
       " 1835,\n",
       " 1839,\n",
       " 1845,\n",
       " 1849,\n",
       " 1853,\n",
       " 1859,\n",
       " 1864,\n",
       " 1868,\n",
       " 1872,\n",
       " 1876,\n",
       " 1880,\n",
       " 1885,\n",
       " 1889,\n",
       " 1893,\n",
       " 1897,\n",
       " 1901,\n",
       " 1905,\n",
       " 1909,\n",
       " 1913,\n",
       " 1919,\n",
       " 1924,\n",
       " 1928,\n",
       " 1932,\n",
       " 1936,\n",
       " 1940,\n",
       " 1944,\n",
       " 1953,\n",
       " 1957,\n",
       " 1961,\n",
       " 1966,\n",
       " 1970,\n",
       " 1974,\n",
       " 1978,\n",
       " 1983,\n",
       " 1987,\n",
       " 1992,\n",
       " 1996,\n",
       " 2000,\n",
       " 2004,\n",
       " 2013,\n",
       " 2022,\n",
       " 2026,\n",
       " 2030,\n",
       " 2034,\n",
       " 2038,\n",
       " 2042,\n",
       " 2046,\n",
       " 2050,\n",
       " 2054,\n",
       " 2058,\n",
       " 2062,\n",
       " 2066,\n",
       " 2070,\n",
       " 2074,\n",
       " 2078,\n",
       " 2082,\n",
       " 2086,\n",
       " 2091,\n",
       " 2095,\n",
       " 2099,\n",
       " 2103,\n",
       " 2107,\n",
       " 2111,\n",
       " 2115,\n",
       " 2119,\n",
       " 2123,\n",
       " 2127,\n",
       " 2131,\n",
       " 2135,\n",
       " 2141,\n",
       " 2145,\n",
       " 2149,\n",
       " 2153,\n",
       " 2157,\n",
       " 2161,\n",
       " 2165,\n",
       " 2169,\n",
       " 2173,\n",
       " 2177,\n",
       " 2182,\n",
       " 2191,\n",
       " 2200,\n",
       " 2209,\n",
       " 2213,\n",
       " 2217,\n",
       " 2221,\n",
       " 2225,\n",
       " 2229,\n",
       " 2233,\n",
       " 2237,\n",
       " 2243,\n",
       " 2247,\n",
       " 2251,\n",
       " 2255,\n",
       " 2259,\n",
       " 2263,\n",
       " 2267,\n",
       " 2272,\n",
       " 2276,\n",
       " 2280,\n",
       " 2284,\n",
       " 2288,\n",
       " 2292,\n",
       " 2296,\n",
       " 2300,\n",
       " 2304,\n",
       " 2308,\n",
       " 2312,\n",
       " 2316,\n",
       " 2320,\n",
       " 2324,\n",
       " 2333,\n",
       " 2337,\n",
       " 2342,\n",
       " 2346,\n",
       " 2350,\n",
       " 2354,\n",
       " 2358,\n",
       " 2363,\n",
       " 2367,\n",
       " 2371,\n",
       " 2375,\n",
       " 2379,\n",
       " 2383,\n",
       " 2387,\n",
       " 2391,\n",
       " 2395,\n",
       " 2399,\n",
       " 2408,\n",
       " 2415,\n",
       " 2423,\n",
       " 2427,\n",
       " 2431,\n",
       " 2440,\n",
       " 2444,\n",
       " 2448,\n",
       " 2452,\n",
       " 2456,\n",
       " 2461,\n",
       " 2466,\n",
       " 2470,\n",
       " 2474,\n",
       " 2478,\n",
       " 2482,\n",
       " 2486,\n",
       " 2491,\n",
       " 2495,\n",
       " 2499,\n",
       " 2504,\n",
       " 2508,\n",
       " 2512,\n",
       " 2516,\n",
       " 2520,\n",
       " 2524,\n",
       " 2528,\n",
       " 2532,\n",
       " 2541,\n",
       " 2545,\n",
       " 2550,\n",
       " 2554,\n",
       " 2558,\n",
       " 2562,\n",
       " 2566,\n",
       " 2570,\n",
       " 2576,\n",
       " 2580,\n",
       " 2584,\n",
       " 2588,\n",
       " 2592,\n",
       " 2596,\n",
       " 2600,\n",
       " 2604,\n",
       " 2609,\n",
       " 2613,\n",
       " 2617,\n",
       " 2621,\n",
       " 2625,\n",
       " 2629,\n",
       " 2633,\n",
       " 2637,\n",
       " 2646,\n",
       " 2650,\n",
       " 2654,\n",
       " 2658,\n",
       " 2662,\n",
       " 2667,\n",
       " 2671,\n",
       " 2675,\n",
       " 2679,\n",
       " 2683,\n",
       " 2692,\n",
       " 2696,\n",
       " 2700,\n",
       " 2705,\n",
       " 2709,\n",
       " 2713,\n",
       " 2717,\n",
       " 2721,\n",
       " 2725,\n",
       " 2729,\n",
       " 2734,\n",
       " 2738,\n",
       " 2742,\n",
       " 2747,\n",
       " 2751,\n",
       " 2755,\n",
       " 2759,\n",
       " 2763,\n",
       " 2767,\n",
       " 2772,\n",
       " 2776,\n",
       " 2780,\n",
       " 2784,\n",
       " 2788,\n",
       " 2792,\n",
       " 2796,\n",
       " 2800,\n",
       " 2804,\n",
       " 2808,\n",
       " 2812,\n",
       " 2816,\n",
       " 2820,\n",
       " 2824,\n",
       " 2828,\n",
       " 2832,\n",
       " 2836,\n",
       " 2840,\n",
       " 2844,\n",
       " 2848,\n",
       " 2852,\n",
       " 2856,\n",
       " 2860,\n",
       " 2864,\n",
       " 2868,\n",
       " 2872,\n",
       " 2876,\n",
       " 2880,\n",
       " 2884,\n",
       " 2888,\n",
       " 2892,\n",
       " 2896,\n",
       " 2900,\n",
       " 2904,\n",
       " 2909,\n",
       " 2913,\n",
       " 2917,\n",
       " 2922,\n",
       " 2927,\n",
       " 2931,\n",
       " 2935,\n",
       " 2939,\n",
       " 2943,\n",
       " 2951,\n",
       " 2956,\n",
       " 2965,\n",
       " 2969,\n",
       " 2973,\n",
       " 2977,\n",
       " 2981,\n",
       " 2989,\n",
       " 2994,\n",
       " 2998,\n",
       " 3005,\n",
       " 3009,\n",
       " 3015,\n",
       " 3019,\n",
       " 3023,\n",
       " 3027,\n",
       " 3031,\n",
       " 3035,\n",
       " 3039,\n",
       " 3043,\n",
       " 3047,\n",
       " 3051,\n",
       " 3060,\n",
       " 3069,\n",
       " 3073,\n",
       " 3077,\n",
       " 3081,\n",
       " 3085,\n",
       " 3094,\n",
       " 3098,\n",
       " 3102,\n",
       " 3106,\n",
       " 3110,\n",
       " 3114,\n",
       " 3118,\n",
       " 3122,\n",
       " 3126,\n",
       " 3130,\n",
       " 3134,\n",
       " 3138,\n",
       " 3142,\n",
       " 3146,\n",
       " 3150,\n",
       " 3154,\n",
       " 3160,\n",
       " 3164,\n",
       " 3168,\n",
       " 3172,\n",
       " 3176,\n",
       " 3180,\n",
       " 3187,\n",
       " 3191,\n",
       " 3195,\n",
       " 3199,\n",
       " 3203,\n",
       " 3207,\n",
       " 3211,\n",
       " 3215,\n",
       " 3219,\n",
       " 3223,\n",
       " 3227,\n",
       " 3231,\n",
       " 3235,\n",
       " 3239,\n",
       " 3243,\n",
       " 3247,\n",
       " 3252,\n",
       " 3257,\n",
       " 3261,\n",
       " 3265,\n",
       " 3269,\n",
       " 3273,\n",
       " 3277,\n",
       " 3286,\n",
       " 3290,\n",
       " 3294,\n",
       " 3298,\n",
       " 3302,\n",
       " 3306,\n",
       " 3310,\n",
       " 3314,\n",
       " 3318,\n",
       " 3322,\n",
       " 3328,\n",
       " 3332,\n",
       " 3336,\n",
       " 3340,\n",
       " 3344,\n",
       " 3352,\n",
       " 3356,\n",
       " 3360,\n",
       " 3364,\n",
       " 3368,\n",
       " 3372,\n",
       " 3376,\n",
       " 3380,\n",
       " 3384,\n",
       " 3388,\n",
       " 3392,\n",
       " 3396,\n",
       " 3400,\n",
       " 3407,\n",
       " 3411,\n",
       " 3415,\n",
       " 3419,\n",
       " 3423,\n",
       " 3427,\n",
       " 3431,\n",
       " 3436,\n",
       " 3440,\n",
       " 3444,\n",
       " 3453,\n",
       " 3458,\n",
       " 3462,\n",
       " 3466,\n",
       " 3470,\n",
       " 3474,\n",
       " 3479,\n",
       " 3483,\n",
       " 3487,\n",
       " 3491,\n",
       " 3497,\n",
       " 3501,\n",
       " 3505,\n",
       " 3509,\n",
       " 3513,\n",
       " 3517,\n",
       " 3521,\n",
       " 3525,\n",
       " 3529,\n",
       " 3533,\n",
       " 3537,\n",
       " 3541,\n",
       " 3545,\n",
       " 3549,\n",
       " 3558,\n",
       " 3563,\n",
       " 3567,\n",
       " 3571,\n",
       " 3575,\n",
       " 3579,\n",
       " 3583,\n",
       " 3588,\n",
       " 3592,\n",
       " 3596,\n",
       " 3601,\n",
       " 3605,\n",
       " 3609,\n",
       " 3613,\n",
       " 3617,\n",
       " 3621,\n",
       " 3626,\n",
       " 3630,\n",
       " 3639,\n",
       " 3644,\n",
       " 3648,\n",
       " 3652,\n",
       " 3657,\n",
       " 3661,\n",
       " 3665,\n",
       " 3669,\n",
       " 3673,\n",
       " 3677,\n",
       " 3681,\n",
       " 3685,\n",
       " 3689,\n",
       " 3698,\n",
       " 3702,\n",
       " 3706,\n",
       " 3710,\n",
       " 3714,\n",
       " 3718,\n",
       " 3722,\n",
       " 3726,\n",
       " 3730,\n",
       " 3734,\n",
       " 3741,\n",
       " 3745,\n",
       " 3749,\n",
       " 3753,\n",
       " 3758,\n",
       " 3767,\n",
       " 3772,\n",
       " 3776,\n",
       " 3781,\n",
       " 3785,\n",
       " 3789,\n",
       " 3793,\n",
       " 3797,\n",
       " 3801,\n",
       " 3805,\n",
       " 3809,\n",
       " 3813,\n",
       " 3817,\n",
       " 3821,\n",
       " 3825,\n",
       " 3829,\n",
       " 3834,\n",
       " 3838,\n",
       " 3843,\n",
       " 3847,\n",
       " 3851,\n",
       " 3855,\n",
       " 3859,\n",
       " 3864,\n",
       " 3868,\n",
       " 3872,\n",
       " 3876,\n",
       " 3880,\n",
       " 3884,\n",
       " 3888,\n",
       " 3892,\n",
       " 3896,\n",
       " 3900,\n",
       " 3904,\n",
       " 3909,\n",
       " 3913,\n",
       " 3918,\n",
       " 3922,\n",
       " 3931,\n",
       " 3938,\n",
       " 3943,\n",
       " 3947,\n",
       " 3951,\n",
       " 3955,\n",
       " 3960,\n",
       " 3964,\n",
       " 3968,\n",
       " 3972,\n",
       " 3976,\n",
       " 3980,\n",
       " 3984,\n",
       " 3988,\n",
       " 3992,\n",
       " 3996,\n",
       " 4000,\n",
       " 4004,\n",
       " 4008,\n",
       " 4012,\n",
       " 4021,\n",
       " 4025,\n",
       " 4029,\n",
       " 4033,\n",
       " 4037,\n",
       " 4041,\n",
       " 4045,\n",
       " 4052,\n",
       " 4056,\n",
       " 4060,\n",
       " 4064,\n",
       " 4068,\n",
       " 4072,\n",
       " 4076,\n",
       " 4080,\n",
       " 4084,\n",
       " 4088,\n",
       " 4092,\n",
       " 4096,\n",
       " 4100,\n",
       " 4105,\n",
       " 4109,\n",
       " 4113,\n",
       " 4118,\n",
       " 4122,\n",
       " 4127,\n",
       " 4131,\n",
       " 4135,\n",
       " 4139,\n",
       " 4145,\n",
       " 4149,\n",
       " 4153,\n",
       " 4160,\n",
       " 4164,\n",
       " 4168,\n",
       " 4172,\n",
       " 4176,\n",
       " 4180,\n",
       " 4184,\n",
       " 4188,\n",
       " 4192,\n",
       " 4196,\n",
       " 4200,\n",
       " 4204,\n",
       " 4208,\n",
       " 4212,\n",
       " 4216,\n",
       " 4220,\n",
       " 4224,\n",
       " 4229,\n",
       " 4233,\n",
       " 4242,\n",
       " 4246,\n",
       " 4250,\n",
       " 4254,\n",
       " 4258,\n",
       " 4263,\n",
       " 4267,\n",
       " 4271,\n",
       " 4275,\n",
       " 4284,\n",
       " 4293,\n",
       " 4298,\n",
       " 4302,\n",
       " 4306,\n",
       " 4310,\n",
       " 4314,\n",
       " 4318,\n",
       " 4322,\n",
       " 4326,\n",
       " 4330,\n",
       " 4334,\n",
       " 4338,\n",
       " 4342,\n",
       " 4346,\n",
       " 4351,\n",
       " 4355,\n",
       " 4359,\n",
       " 4363,\n",
       " 4367,\n",
       " 4371,\n",
       " 4375,\n",
       " 4379,\n",
       " 4383,\n",
       " 4387,\n",
       " 4393,\n",
       " 4397,\n",
       " 4401,\n",
       " 4405,\n",
       " 4409,\n",
       " 4413,\n",
       " 4418,\n",
       " 4422,\n",
       " 4426,\n",
       " 4430,\n",
       " 4435,\n",
       " 4439,\n",
       " 4443,\n",
       " 4447,\n",
       " 4451,\n",
       " 4455,\n",
       " 4459,\n",
       " 4463,\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history['nb_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'reward')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['nb_steps'],hist.history['episode_reward'])\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "choice(2) - conversions: [0, 0.30971084184322467], reactors: [1]\n",
      "choice(3) - conversions: [0, 0.30971084184322467, 0.6700337959849747], reactors: [1, 0]\n",
      "choice(4) - conversions: [0, 0.30971084184322467, 0.6700337959849747, 0.9613537829016082], reactors: [1, 0, 1]\n",
      "choice(5) - conversions: [0, 0.30971084184322467, 0.6700337959849747, 0.9613537829016082, 1], reactors: [1, 0, 1, 1]\n",
      "Episode 1: reward: 1.000, steps: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a72f565c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, visualize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 0, 1, 1],\n",
       " [0, 0.30971084184322467, 0.6700337959849747, 0.9613537829016082, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reactor_seq, env.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [] \n",
    "x = []\n",
    "CSTR = []\n",
    "PFR = []\n",
    "for i in range(0, 50):\n",
    "    x.append(i/50)\n",
    "    CSTR.append(env.equation_solver(0, x[i]))\n",
    "    PFR.append(env.equation_solver(1, x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'x next')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_sim = env.X[0:-1]\n",
    "choice = env.X[1:]\n",
    "plt.plot(x, CSTR)\n",
    "plt.plot(x, PFR)\n",
    "plt.plot(x_sim, choice, '--go')\n",
    "plt.legend([\"CSTR\", \"PFR\", \"CHOICE\"])\n",
    "plt.xlabel(\"x previous\")\n",
    "plt.ylabel(\"x next\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "??dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "??env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "??dqn.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "??dqn.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "??SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
