{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Distillation_disc_cont import simulator\n",
    "from Actor_Critic_lstm import Agent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.load_session('Agent.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0930 16:59:15.710533  5008 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0930 16:59:15.730479  5008 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0930 16:59:15.735663  5008 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "D:\\Github\\RL-Process-Design\\Discrete and Continuous\\Actor_Critic_lstm.py:59: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  actor = Model(input = [input, delta], output = [policy_discrete_probs, policy_continuous])\n",
      "W0930 16:59:16.043371  5008 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0930 16:59:16.062479  5008 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc1 (Dense)                     (None, 20)           140         Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fc2 (Dense)                     (None, 20)           420         fc1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 20, 1)        0           fc2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20)           1760        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "policy_output (Dense)           (None, 1)            21          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "discrete_action_output (Dense)  (None, 5)            105         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "policy_output_actual_value (Lam (None, 1)            0           policy_output[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,446\n",
      "Trainable params: 2,446\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Github\\RL-Process-Design\\Discrete and Continuous\\Actor_Critic_lstm.py:62: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  critic = Model(input = [input], output = [Values])\n",
      "D:\\Github\\RL-Process-Design\\Discrete and Continuous\\Actor_Critic_lstm.py:67: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  policy = Model(input = [input], output = [policy_discrete_probs, policy_continuous])\n"
     ]
    }
   ],
   "source": [
    "env = simulator()\n",
    "agent = Agent(env = env, alpha = 1e-6, beta = 1e-7, gamma = 0.99)\n",
    "agent.actor.summary()\n",
    "#agent.critic.summary()\n",
    "#agent.policy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 16:59:49.022184  5008 deprecation_wrapper.py:119] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0930 16:59:49.567752  5008 deprecation.py:323] From C:\\Users\\meatrobot\\Anaconda3\\envs\\RL\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "D:\\Github\\RL-Process-Design\\Discrete and Continuous\\Distillation_disc_cont.py:63: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  LK_D = tops[Light_Key]/sum(tops)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, avg_score -12507.20345099872, last action (2, array([[0.94902276]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Github\\RL-Process-Design\\Discrete and Continuous\\Distillation_disc_cont.py:64: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  LK_B = bots[Light_Key]/sum(bots)\n",
      "D:\\Github\\RL-Process-Design\\Discrete and Continuous\\Distillation_disc_cont.py:74: RuntimeWarning: invalid value encountered in log\n",
      "  N =  np.log(LK_D/(1-LK_D) * (1-LK_B)/LK_B)/np.log(self.relative_volatility[Light_Key])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100, avg_score -6018.9484413725695, last action (2, array([[0.999]], dtype=float32))\n",
      "episode 200, avg_score -9189.837547078818, last action (2, array([[0.16456735]]))\n",
      "episode 300, avg_score -10680.380330005339, last action (2, array([[-0.6109293]]))\n",
      "episode 400, avg_score -6542.204614947083, last action (2, array([[0.52066563]]))\n",
      "episode 500, avg_score -10125.22753459931, last action (2, array([[0.75844605]]))\n",
      "episode 600, avg_score -12590.265065797605, last action (2, array([[0.999]], dtype=float32))\n",
      "episode 700, avg_score -11802.435072919778, last action (0, array([[0.22936471]]))\n",
      "episode 800, avg_score -11944.083857722835, last action (2, array([[0.999]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "score_history = []\n",
    "num_episodes = int(1e4)\n",
    "for i in range(num_episodes):\n",
    "\tdone = False\n",
    "\t\n",
    "\tstate = env.reset()\n",
    "\tscore = 0\n",
    "\t\n",
    "\twhile not done:\n",
    "\t\taction = agent.choose_action_epsgreedy(state, i, num_episodes)\n",
    "\t\tnext_state, reward, done, info = env.step(action)\n",
    "\t\tagent.learn(state,  action, reward, next_state, done)\n",
    "\t\tstate = next_state\n",
    "\t\tscore += reward\n",
    "\t\n",
    "\tscore_history.append(score)\n",
    "\tavg_score = np.mean(score_history[-100:]) #average of last 100 scores\n",
    "\tif i%100 == 0:\n",
    "\t\tprint(f'episode {i}, avg_score {avg_score}, last action {action}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = np.arange(num_episodes)\n",
    "smoothed_rews = running_mean(score_history, 10)\n",
    "plt.plot(episodes[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(episodes, score_history,color='grey', alpha=0.3)\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend([\"avg reward\", \"reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "while done == False:\n",
    "    state = state[np.newaxis, :]\n",
    "    action_discrete, action_continuous = agent.policy.predict(state)\n",
    "    action_discrete = np.argmax(action_discrete)\n",
    "    action = action_discrete, action_continuous\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f'action LK: {action_discrete}, action split: {action_continuous[0]}, reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stream_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_discrete, action_continuous = agent.policy.predict(state[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump_session('Agent.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
