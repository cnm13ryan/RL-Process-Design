{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from Distillation_disc_cont import Simulator\n",
    "from memory import Memory\n",
    "from Hybrid_paper import Agent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.load_session('Agent.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make agent and define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3\n",
    "beta = 1e-4 \n",
    "gamma = 0.97\n",
    "\n",
    "num_episodes = int(1e4)\n",
    "memory_size = int(1e5)\n",
    "batch_size = 20\n",
    "pretrain_eps = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_40\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_state_DQN (InputLayer)    (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_param_DQN (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense1_state_DQN (Dense)        (None, 30)           210         Input_state_DQN[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense1_param_DQN (Dense)        (None, 30)           60          input_param_DQN[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 30)           0           dense1_state_DQN[0][0]           \n",
      "                                                                 dense1_param_DQN[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense1_DQN (Dense)              (None, 30)           930         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2_DQN (Dense)              (None, 30)           930         dense1_DQN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "discrete_action_output (Dense)  (None, 5)            155         dense2_DQN[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,285\n",
      "Trainable params: 2,285\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Github\\RL-Process-Design\\Discrete and Continuous\\Hybrid_paper.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"In..., outputs=Tensor(\"po...)`\n",
      "  actor_param = Model(input = input_state, output = policy_continuous)\n",
      "D:\\Github\\RL-Process-Design\\Discrete and Continuous\\Hybrid_paper.py:60: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  actor_DQN = Model(input = [input_state_DQN, input_param_DQN], output = [policy_discrete_probs])\n"
     ]
    }
   ],
   "source": [
    "env = Simulator()\n",
    "memory = Memory(max_size = memory_size)\n",
    "agent = Agent(env = env, alpha = alpha, beta = beta, gamma = gamma, layer_size = 30)\n",
    "agent.actor_DQN.summary()\n",
    "#agent.actor_param.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the experience memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for ep in range(pretrain_eps):\n",
    "    action = env.discrete_action_space.sample(), env.continuous_action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(env.observation_space.shape)\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "    else:\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, avg_score -1385.3444752751188, last action (0, array([[-0.47835606]]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-202-0090024f4220>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Github\\RL-Process-Design\\Discrete and Continuous\\Hybrid_paper.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, state, action, reward, next_state, done, batch_size, verbose)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m#print(target.shape, critic_value.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;31m#print(state.shape, delta.shape, actions_discrete.shape, action_continuous.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#verbose = 0 stops outputs from displaying\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_DQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m             \u001b[0mfit_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m         \u001b[0mfit_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    510\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[0;32m    511\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                         loss=self.total_loss)\n\u001b[0m\u001b[0;32m    513\u001b[0m                 updates = (self.updates +\n\u001b[0;32m    514\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             raise ValueError('An operation has `None` for gradient. '\n\u001b[0m\u001b[0;32m     92\u001b[0m                              \u001b[1;34m'Please make sure that all of your ops have a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                              \u001b[1;34m'gradient defined (i.e. are differentiable). '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval."
     ]
    }
   ],
   "source": [
    "score_history = []\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action_epsgreedy(state, i, num_episodes)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        #agent.learn(state,  action, reward, next_state, done)\n",
    "        score += reward\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "    \n",
    "    score_history.append(score)\n",
    "    next_state = np.zeros(state.shape)\n",
    "    memory.add((state, action, reward, next_state))\n",
    "    \n",
    "    avg_score = np.mean(score_history[-100:]) #average of last 100 scores\n",
    "    if i%100 == 0:\n",
    "        print(f'episode {i}, avg_score {avg_score}, last action {action}')\n",
    "        \n",
    "    batch = memory.sample(batch_size)\n",
    "    states = np.array([each[0] for each in batch])\n",
    "    actions = np.array([each[1] for each in batch])\n",
    "    rewards = np.array([each[2] for each in batch])\n",
    "    next_states = np.array([each[3] for each in batch])\n",
    "    \n",
    "    agent.learn(states,  actions, rewards, next_states, done, batch_size, verbose = 1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which layer is without gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'running_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-ef458c681701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msmoothed_rews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmoothed_rews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmoothed_rews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'grey'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"steps\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'running_mean' is not defined"
     ]
    }
   ],
   "source": [
    "episodes = np.arange(num_episodes)\n",
    "smoothed_rews = running_mean(score_history, 10)\n",
    "plt.plot(episodes[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(episodes, score_history,color='grey', alpha=0.3)\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend([\"avg reward\", \"reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action LK: 2, action split: [0.5], reward: -0.12775379014095772\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n",
      "action LK: 2, action split: [0.5], reward: -100\n"
     ]
    }
   ],
   "source": [
    "#test agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    state = state[np.newaxis, :]\n",
    "    action_discrete, action_continuous = agent.policy.predict(state)\n",
    "    action_discrete = np.argmax(action_discrete)\n",
    "    action = action_discrete, action_continuous\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f'action LK: {action_discrete}, action split: {action_continuous[0]}, reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([9.1, 6.8, 9.1, 6.8, 6.8, 6.8]),\n",
       " array([9.1       , 6.8       , 4.55000019, 3.4000001 , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 4.54999981, 3.3999999 , 6.8       ,\n",
       "        6.8       ]),\n",
       " array([9.1       , 6.8       , 2.2750001 , 1.70000005, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 2.2750001 , 1.70000005, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 2.27499986, 1.69999993, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 2.27499995, 1.69999998, 6.8       ,\n",
       "        6.8       ]),\n",
       " array([9.1       , 6.8       , 1.13750005, 0.85000002, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 1.13750005, 0.85000002, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 1.13750005, 0.85000002, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 1.13750005, 0.85000002, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 1.13749993, 0.84999996, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 1.13749993, 0.84999996, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 1.13749993, 0.84999996, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 1.13750002, 0.85000001, 6.8       ,\n",
       "        6.8       ]),\n",
       " array([9.1       , 6.8       , 0.56875002, 0.42500001, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.56875002, 0.42500001, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.56875002, 0.42500001, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.56875002, 0.42500001, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.56875002, 0.42500001, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.56875002, 0.42500001, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.56875002, 0.42500001, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.        , 0.        , 0.56875002, 0.42500001, 0.        ,\n",
       "        0.        ])]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.stream_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_discrete, action_continuous = agent.policy.predict(state[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump_session('Agent.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
